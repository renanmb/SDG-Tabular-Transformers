{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "##\n",
    "## Copyright (C) 2022 NVIDIA Corporation.  All rights reserved.\n",
    "##\n",
    "## NVIDIA Sample Code\n",
    "##\n",
    "## Please refer to the NVIDIA end user license agreement (EULA) associated\n",
    "## with this source code for terms and conditions that govern your use of\n",
    "## this software. Any use, reproduction, disclosure, or distribution of\n",
    "## this software and related documentation outside the terms of the EULA\n",
    "## is strictly prohibited.\n",
    "##\n",
    "############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 1. Generate synthetic credit card transactions\n",
    "\n",
    "We come to the next step of this workshop - generate synthetic credit card transactions!\n",
    "\n",
    "First let's load the trained model weights, put the Megatron GPT in inference mode and start the text generation server. All of these can be done by running the `run_data_gen_server.sh` script.\n",
    "\n",
    "The text generation server accepts REST API request to send the generated text in the response. Let's use the following Python code to generate some transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_path = 'credit_card_coder.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from coder.column_code import ColumnTokenizer\n",
    "import pickle\n",
    "\n",
    "with open(vocabulary_path, 'rb') as handle:\n",
    "        cc: ColumnTokenizer = pickle.load(handle)\n",
    "\n",
    "TOKENS_PER_ROW = sum(cc.sizes) + 1\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "PORT_NUM = 5000\n",
    "NUM_ROWS = 30\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "\n",
    "def request_data(data):\n",
    "    resp = requests.put('http://localhost:{}/generate'.format(PORT_NUM),\n",
    "                        data=json.dumps(data), headers=HEADERS)\n",
    "    sentences = resp.json()['sentences']\n",
    "    return sentences\n",
    "\n",
    "\n",
    "# generate the inital transactions unconditionally\n",
    "data = {\n",
    "    \"sentences\": [\"\"] * BATCH_SIZE,\n",
    "    \"tokens_to_generate\": NUM_ROWS * TOKENS_PER_ROW,\n",
    "    \"temperature\": 1.0,\n",
    "    \"add_BOS\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|endoftext|>1399|0|2015|8|26|16|36|162.94210326270257|Online Transaction|8099188931555596779|ONLINE|None||7996|None|0\n",
      "1399|0|2015|8|26|16|59|120.0|Chip Transaction|-4282466774399734331|Channelview|TX|77530|4829|None|0\n",
      "1399|0|2015|8|27|8|13|139.99952433243243|Chip Transaction|-4282466774399734331|Channelview|TX|77530|4829|None|0\n",
      "1399|0|2015|8|27|8|20|2.4024739|Chip Transaction|-5475680618560174533|Channelview|TX|77530|5942|None|0\n",
      "1399|0|2015|8|29|7|19|45.85496951999999|Chip Transaction|-4534432520820813184|Alvin|TX|77511|7230|None|0\n",
      "1399|0|2015|8|29|7|47|139.99952433243243|Chip Transaction|-4282466774399734331|Channelview|TX|77530|4829|None|0\n",
      "1399|0|2015|8|29|8|23|139.99952433243243|Chip Transaction|-4282466774399734331|Channelview|TX|77530|4829|None|0\n",
      "1399|0|2015|8|30|13|31|84.06869268378381|Chip Transaction|1799189980464955940|Alvin|TX|77511|5499|None|0\n",
      "1399|0|2015|8|30|13|37|46.2293459|Chip Transaction|1799189980464955940|Alvin|TX|77511|5499|None|0\n",
      "1399|0|2015|8|30|13|39|-84.00002099999999|Chip Transaction|1799189980464955940|Alvin|TX|77511|5499|None|0\n",
      "1399|0|2015|8|31|0|40|50.06264254000001|Chip Transaction|-2312290263703610363|Alvin|TX|77511|5661|None|0\n",
      "1399|0|2015|8|31|7|41|139.99952433243243|Chip Transaction|-4282466774399734331|Channelview|TX|77530|4829|None|0\n",
      "1399|0|2015|8|31|8|18|139.99952433243243|Chip Transaction|-4282466774399734331|Channelview|TX|77530|4829|None|0\n",
      "1399|0|2015|8|31|21|40|7.36227905189189|Swipe Transaction|-2466420595087728725|Alvin|TX|77511|7538|None|0\n",
      "1399|0|2015|9|1|8|1|80.0|Chip Transaction|-4282466774399734331|Channelview|TX|77530|4829|None|0\n",
      "1399|0|2015|9|2|11|59|22.07994176|Chip Transaction|-5467922351692495955|Alvin|TX|77511|5912|None|0\n",
      "1399|0|2015|9|2|21|21|7.215696960000001|Chip Transaction|3952145593743244256|Alvin|TX|77511|7538|None|0\n",
      "1399|0|2015|9|3|8|6|139.99952433243243|Chip Transaction|-4282466774399734331|Channelview|TX|77530|4829|None|0\n",
      "1399|0|2015|9|3|15|42|17.58999|Swipe Transaction|97032797689821735|Alvin|TX|77511|5411|None|0\n",
      "1399|0|2015|9|4|8|15|139.99952433243243|Chip Transaction|-4282466774399734331|Channelview|TX|77530|4829|None|0\n",
      "1399|0|2015|9|5|8|16|80.0|Chip Transaction|-4282466774399734331|Channelview|TX|77530|4829|None|0\n",
      "1399|0|2015|9|6|7|58|120.0|Chip Transaction|-4282466774399734331|Channelview|TX|77530|4829|None|0\n",
      "1399|0|2015|9|6|8|12|176.31773722000017|Chip Transaction|-4282466774399734331|Channelview|TX|77530|4829|None|0\n",
      "1399|0|2015|9|7|8|12|100.0|Chip Transaction|-4282466774399734331|Channelview|TX|77530|4829|Insufficient Balance,|0\n",
      "1399|0|2015|9|7|8|24|120.0|Chip Transaction|-4282466774399734331|Channelview|TX|77530|4829|None|0\n",
      "1399|0|2015|9|7|11|11|78.51288616|Chip Transaction|-9010667332435666302|Alvin|TX|77511|7995|None|0\n",
      "1399|0|2015|9|7|11|59|21.302136100000002|Chip Transaction|6661973303171003879|Alvin|TX|77511|5211|None|0\n",
      "1399|0|2015|9|7|15|42|13.731409139999998|Chip Transaction|97032797689821735|Alvin|TX|77511|5411|None|0\n",
      "1399|0|2015|9|8|8|12|120.0|Chip Transaction|-4282466774399734331|Channelview|TX|77530|4829|None|0\n",
      "1399|0|2015|9|9|7|59|120.0|Chip Transaction|-4282466774399734331|Channelview|TX|77530|4829|None|0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentences = request_data(data)\n",
    "\n",
    "for i in range(BATCH_SIZE):\n",
    "    s = sentences[i]\n",
    "    print(s)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 2. What just happened?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "The above code only generated `NUM_ROWS` of transactions starting from the an empty string. The `BATCH_SIZE` parameter allows the generation to run in parallel to generate `NUM_ROWS` per batch. Thus, the number of generated transactions in the single request above is `BATCH_SIZE * NUM_ROWS`. Multiplying `NUM_ROWS * TOKENS_PER_ROW` allows to specify the number of generated rows. \n",
    "\n",
    "If generation gives an `Out of Memory error` (see the output from the notebook where the server is run), consider decreasing the `BATCH_SIZE` or the `NUM_ROWS`. A safe value for the `BATCH_SIZE` should be the `MICRO_BATCH_SIZE` defined in the `pretrain_step.sh` as this was used to train the model. \n",
    "\n",
    "If a longer sequence is needed, we can run the inference conditioned on the past transactions in a sliding window fashion. For example, first the model generates `[A, B, C, D, E]` transactions conditioned on an `<|endoftext|>` token. Then, it conditions on `[D, E]` and generates `[D, E, F, G, H]`. Once the long sequence comes to the end indicated by the special `<|endoftext|>` token, it will keep generating new transactions for a different user. For example, after generating `[X, Y, Z, <|endoftext|>]`, the model will generate `[Z, <|endoftext|>, A', B', C']` in the next iteration, where `A', B', C'` are transactions for a different user and not dependent on the former user's transaction `Z`.\n",
    "\n",
    "In more practical terms, suppose we are interested in this past Christmas' transactions for Emily Smith, and we want to know how this will effect Emily's purchasing in January. We can pass the Christmas transactions to GPT and it will \"condition\" the output based on our input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 3. How to pass the most \"context\" to our model?\n",
    "\n",
    "To obtain the maximum context use the `SEQ_LEN` variable, which tells us the max sequence length for our model. Since we already calculated the number of tokens per row, we can calculate how many previous transactions (rows) to pass into the model, leaving one row to generate.\n",
    "\n",
    "For example, if our sequence length is `5`, then we can pass `[A, B, C, D]` as the context to conditionally generate `[A, B, C, D, E]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 255)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOKENS_PER_ROW = sum(cc.sizes) + 1  # add 1 for newline char to separate each row\n",
    "SEQ_LEN=6144\n",
    "\n",
    "# subtract 2 because of extra endoftext token, which is in first row only, and we need to leave space for an extra generated row\n",
    "NUM_ROWS = SEQ_LEN//TOKENS_PER_ROW\n",
    "NUM_CONTEXT_ROWS = NUM_ROWS - 1\n",
    "\n",
    "# we will need to remove any partial row that will be generated (the last row).\n",
    "TOKENS_PER_ROW, NUM_CONTEXT_ROWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code implements the ideas above for conditional generation and can be used to generate massive number of transactions in long sequences.\n",
    "\n",
    "Again, we start from unconditional generation to \"seed\" the context, and then keep feeding this historical context as a rolling window. \n",
    "\n",
    "Alternatively, we can provide real transactions as the context for the model to generate new synthetic transactions. A similar principle applies for time series forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cudf\n",
    "\n",
    "out_data_fp = './data/card_transactions_fixed.parquet'\n",
    "df = cudf.read_parquet(out_data_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "user0 = df.loc[df.user == 0]\n",
    "user0 = user0.fillna('None')\n",
    "# recall this from the first notebook if you had actually used EXCLUDED_COLS\n",
    "EXCLUDED_COLS = []\n",
    "COLUMNS = [col for col in user0.columns if col not in EXCLUDED_COLS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/cudf/io/json.py:108: UserWarning: Using CPU via Pandas to write JSON dataset, this may be GPU accelerated in the future\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ENDOFTEXT = '<|endoftext|>'\n",
    "DELIMITER = '|'\n",
    "\n",
    "\n",
    "def make_context(dframe, columns, delimiter):\n",
    "    dframe = dframe[columns].astype(str)\n",
    "    result = dframe[[dframe.columns[0]]].copy()\n",
    "    result.columns = ['Out']\n",
    "    cols = dframe.columns[1:]\n",
    "    for col in cols:\n",
    "        result['Out'] = result['Out'].str.cat(dframe[col], sep=delimiter)\n",
    "\n",
    "    return '\\n'.join(result['Out'].to_json(orient='values')[2:-2].replace('\\\\', '').split('\",\"'))\n",
    "\n",
    "context = make_context(user0, COLUMNS, DELIMITER).split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19963, 255)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(context), len(context[:NUM_CONTEXT_ROWS])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 3.1 Example - One conditional generation iteration: let's pass this context and generate new transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conditional generation of the transactions.\n",
    "BATCH_SIZE = 2\n",
    "data = {\n",
    "    \"sentences\": ['\\n'.join(context[:NUM_CONTEXT_ROWS])] * BATCH_SIZE,\n",
    "    \"tokens_to_generate\": NUM_ROWS * TOKENS_PER_ROW,\n",
    "    \"temperature\": 1.0,\n",
    "    \"add_BOS\": True\n",
    "}\n",
    "\n",
    "sentences = request_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of `sentences` variable: 2\n",
      "length of a returned sentence: 256\n"
     ]
    }
   ],
   "source": [
    "# some properties of the sentences variable:\n",
    "print(\"length of `sentences` variable: {}\\n\"\n",
    "       \"length of a returned sentence: {}\".format(len(sentences), len(sentences[0].split('\\n'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.1a Notice how we are able to recover the original passed transactions. We can easily `round` the amounts to two decimal points if desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_amount(row_list, col_index, delimiter):\n",
    "    # Round the amounts to two decimal points. Repeat for each row, if desired.\n",
    "    rows = []\n",
    "    for i in row_list:\n",
    "        split_row = i.split(delimiter)\n",
    "        amount = str(round(float(split_row[col_index]), 2))\n",
    "        split_row[col_index] = amount\n",
    "        rows.append(delimiter.join(split_row))\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0|0|2002|9|1|6|21|134.09|Swipe Transaction|3527213246127876953|La Verne|CA|91750|5300|None|0\n",
      "0|0|2002|9|1|6|42|38.48|Swipe Transaction|-727612092139916043|Monterey Park|CA|91754|5411|None|0\n",
      "0|0|2002|9|2|6|22|120.34|Swipe Transaction|-727612092139916043|Monterey Park|CA|91754|5411|None|0\n",
      "\n",
      "0|0|2002|9|1|6|21|134.09|Swipe Transaction|3527213246127876953|La Verne|CA|91750|5300|None|0\n",
      "0|0|2002|9|1|6|42|38.48|Swipe Transaction|-727612092139916043|Monterey Park|CA|91754|5411|None|0\n",
      "0|0|2002|9|2|6|22|120.34|Swipe Transaction|-727612092139916043|Monterey Park|CA|91754|5411|None|0\n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join(round_amount(context[:3], COLUMNS.index('amount'), DELIMITER)), '\\n\\n', \n",
    "      '\\n'.join(round_amount(sentences[0].replace(ENDOFTEXT, '').split('\\n')[:3], COLUMNS.index('amount'), DELIMITER)),\n",
    "      sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 4. Let's check out our generated transactions, compare `context` with the `sentences` variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original context:\n",
      "0|3|2020|2|28|6|53|34.11|Swipe Transaction|-34551508091458520|La Verne|CA|91750|5912|None|0\n",
      "0|2|2020|2|28|7|36|41.05|Chip Transaction|5817218446178736267|La Verne|CA|91750|5912|None|0\n",
      "\n",
      "Sentences (this length is based on batch size):\n",
      "0|0|2002|12|2|20|19|78.32|Swipe Transaction|-2744911404133435018|Chicago|IL|60645|5812|None|0\n",
      "0|0|2002|12|2|23|22|127.0|Swipe Transaction|-6406662083475903219|Chicago|IL|60643|3390|None|0\n",
      "0|0|2002|12|2|23|48|211.0|Swipe Transaction|-7807051024009846392|Peoria|IL|61604|3684|None|0\n",
      "0|0|2002|12|3|13|33|10.6|Swipe Transaction|-4733023138943446282|Chicago|IL|60643|5812|None\n",
      "\n",
      "0|0|2002|12|2|20|19|78.32|Swipe Transaction|-2744911404133435018|Chicago|IL|60645|5812|None|0\n",
      "0|0|2002|12|2|23|22|127.0|Swipe Transaction|-6406662083475903219|Chicago|IL|60643|3390|None|0\n",
      "0|0|2002|12|2|23|48|211.0|Swipe Transaction|-7807051024009846392|Peoria|IL|61604|3684|None|0\n",
      "0|0|2002|12|4|13|25|12.04|Swipe Transaction|-4733023138943446282|Chicago|IL|60643|5812|None\n"
     ]
    }
   ],
   "source": [
    "# The last two rows are the generated transaction, cross reference with the passed in context\n",
    "\n",
    "print('Original context:\\n',\n",
    "      '\\n'.join(round_amount(context[-2:], COLUMNS.index('amount'), DELIMITER)), '\\n\\n',\n",
    "      'Sentences (this length is based on batch size):\\n',\n",
    "      '\\n'.join(round_amount(sentences[0].rsplit('\\n', 4)[1:], COLUMNS.index('amount'), DELIMITER)), '\\n\\n',\n",
    "      '\\n'.join(round_amount(sentences[1].rsplit('\\n', 4)[1:], COLUMNS.index('amount'), DELIMITER)),\n",
    "      sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example criterion to check if there is a complete or partial final row\n",
    "len(sentences[0].rsplit('\\n', 1)[-1].split(DELIMITER)) == len(COLUMNS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the last row in each `sentences` item is a partial row, remove it to only save complete rows. Adjusting the `SEQ_LEN` variable prior to training the model can help reduce the amount of extra text. The reason we discard the partial row is because we created the TabularTokenizer in such a fashion as to iterate over complete rows. A more recent implementation in <a href=\"https://github.com/NVIDIA/NeMo\">Nemo-Megatron</a> addresses this. In our iteration example below, this is taken into account by reducing the `NUM_CONTEXT_ROWS` by 1 to ensure a complete row is generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 5. Iteratively Generating transactions\n",
    "\n",
    "One strategy to iteratively generate transactions is as follows:\n",
    "- Generate transactions unconditionally. \n",
    "- The first generated transactions are used on the second iteration as the context to generate 1 additional row\n",
    "- Use a rolling window on the context in subsequent iterations to generate 1 new row per iteration\n",
    "\n",
    "If an `<|endoftext|>` token appears during the generation, the model will have finished generating transactions for that particular user and begun generating transactions for another user. It is up to the developer whether to keep track of the different users generated and segment the generated user transactions whether\n",
    "- the `user A` was generated in a different batch, assuming `BATCH_SIZE` > 1\n",
    "- in a batch, the `user A` was generated, then another `user B` generated, then later `user A` was generated again\n",
    "\n",
    "and counting the two instances of `user A` as one or two distinct users.\n",
    "\n",
    "In the example below, we will not do any user segmentation, however, we did do this in practice. The synthetic dataset provided in the Evaluation notebook includes this segmentation, but we merged all user instances together for the analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from coder.column_code import ColumnTokenizer\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "with open(vocabulary_path, 'rb') as handle:\n",
    "        cc: ColumnTokenizer = pickle.load(handle)\n",
    "\n",
    "NITERATIONS = 10 # MAKE THIS A LARGER NUMBER FOR LONGER GENERATION\n",
    "\n",
    "BATCH_SIZE = 1  # Increasing batch size will consume more GPU memory. Adjust to fit on your GPU.\n",
    "TOKENS_PER_ROW = sum(cc.sizes) + 1\n",
    "SEQ_LEN=6144  # From the model architecture\n",
    "# subtract 2 because of extra endoftext token, which is in first row only, and we need to leave space for an extra generated row\n",
    "NUM_ROWS = SEQ_LEN//TOKENS_PER_ROW\n",
    "NUM_CONTEXT_ROWS = NUM_ROWS - 1\n",
    "\n",
    "# Request params\n",
    "PORT_NUM = 5000\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "#NAME OF THE FILES\n",
    "WRITE_FILES = False\n",
    "PREFIX_NAME = 'synthetic'\n",
    "files = []\n",
    "\n",
    "\n",
    "def request_data(data):\n",
    "    resp = requests.put('http://localhost:{}/generate'.format(PORT_NUM),\n",
    "                        data=json.dumps(data), headers=HEADERS)\n",
    "    sentences = resp.json()['sentences']\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def get_condition_text(sentences, NUM_CONTEXT_ROWS):\n",
    "    condition_text = ['\\n'.join([ss for ss in s.split('\\n')[-NUM_CONTEXT_ROWS:]]) for s in sentences]\n",
    "    return condition_text\n",
    "\n",
    "\n",
    "def get_extra_text(sentences, NUM_CONTEXT_ROWS):\n",
    "    extra_text = ['\\n'.join([ss for ss in s.split('\\n')[NUM_CONTEXT_ROWS:]]) for s in sentences]\n",
    "    return extra_text\n",
    "\n",
    "\n",
    "def check_last_full_row(sentences, DELIMITER, COLUMNS):\n",
    "    \"\"\"removes last row if it is a partial row\"\"\"\n",
    "    if isinstance(sentences, list) and len(sentences[0].rsplit('\\n', 1)[-1].split(DELIMITER)) == len(COLUMNS):\n",
    "        return sentences, True\n",
    "    s = []\n",
    "    for sentence in sentences:\n",
    "        s.append(sentence.rsplit('\\n', 1)[0])\n",
    "    return s, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# generate the inital transactions \n",
    "data = {\n",
    "    \"sentences\": [\"\"] * BATCH_SIZE,\n",
    "    \"tokens_to_generate\": NUM_ROWS * TOKENS_PER_ROW,\n",
    "    \"temperate\": 1.0,\n",
    "    \"add_BOS\": True\n",
    "}\n",
    "\n",
    "sentences = request_data(data)\n",
    "\n",
    "# round the amount data\n",
    "sentences = ['\\n'.join(round_amount(s.replace(ENDOFTEXT,'').split('\\n'), \n",
    "                                    COLUMNS.index('amount'),\n",
    "                                    DELIMITER)\n",
    "                      ) \n",
    "             for s in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(get_condition_text(sentences, NUM_CONTEXT_ROWS)[0].split('\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:18<00:00,  1.81s/it]\n"
     ]
    }
   ],
   "source": [
    "if WRITE_FILES:\n",
    "    for i in range(BATCH_SIZE):\n",
    "        files.append(open(\"{}_{}.txt\".format(PREFIX_NAME, i), 'w'))\n",
    "        s = sentences[i]\n",
    "        files[i].write(s.replace('<|endoftext|>', '\\n'))\n",
    "else:\n",
    "    generated_rows = [i.split('\\n') for i in sentences]\n",
    "\n",
    "# generate the transactions conditioned on the previous ones\n",
    "pbar = tqdm(range(NITERATIONS))\n",
    "\n",
    "for iteration in pbar:\n",
    "    # get conditional text and prepare payload\n",
    "    condition_text = get_condition_text(sentences, NUM_CONTEXT_ROWS)\n",
    "\n",
    "    data = {\n",
    "        \"sentences\": condition_text,\n",
    "        \"tokens_to_generate\": NUM_ROWS * TOKENS_PER_ROW,\n",
    "        \"temperate\": 1.0,\n",
    "        \"add_BOS\": False\n",
    "    }\n",
    "\n",
    "    # request new generated data\n",
    "    sentences = request_data(data)\n",
    "    \n",
    "    # round the amount data\n",
    "    sentences = ['\\n'.join(round_amount(s.replace(ENDOFTEXT,'').split('\\n'), \n",
    "                                        COLUMNS.index('amount'),\n",
    "                                        DELIMITER)\n",
    "                          ) \n",
    "                 for s in sentences]\n",
    "    \n",
    "    sentences, is_last_full = check_last_full_row(sentences, DELIMITER, COLUMNS)\n",
    "    \n",
    "    if not is_last_full:\n",
    "        # reduce context by 1 row to provide room for generation\n",
    "        print('adjusting NUM_CONTEXT_ROWS')\n",
    "        NUM_CONTEXT_ROWS -= 1\n",
    "        assert NUM_CONTEXT_ROWS > 0\n",
    "    \n",
    "    extra_text = get_extra_text(sentences, NUM_CONTEXT_ROWS)\n",
    "    \n",
    "    if WRITE_FILES:\n",
    "        for i in range(BATCH_SIZE):\n",
    "            s = extra_text[i]\n",
    "            files[i].write(s.replace('<|endoftext|>', '\\n'))\n",
    "            files[i].flush()\n",
    "    else:\n",
    "        for i in range(BATCH_SIZE):\n",
    "            generated_rows[i].append(extra_text[i])\n",
    "    \n",
    "if WRITE_FILES:\n",
    "    for i in range(BATCH_SIZE):\n",
    "        files[i].close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div><font size=\"4\">That's it! So far, we have learned how to preprocess our raw data and train a Megatron GPT model to generate synthetic data.</font></div>\n",
    "\n",
    "<div><font size=\"4\">In the next notebook, we will evaluate our synthetic data and compare it against our real data.</font></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
