{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58efa4e8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# RUN INFERENCE SERVER\n",
    "\n",
    "\n",
    "The cell below is a startup script for our inference server. It will load the model checkpoint we have chosen and allow us to send `PUT` requests to the server to generate synthetic tabular data! The script is loaded shown below. The server is started on port 5000 by default.\n",
    "\n",
    "It may take a few moments to load the checkpoint and start the server...\n",
    "\n",
    "<strong><u>Do not shut down the kernel for this notebook until completed with `3b_Inference.ipynb`</u></strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c426692a",
   "metadata": {},
   "source": [
    "The pretraining will save the checkpoint files periodically at the `CHECKPOINT_PATH`. `save-interval` in the script controls the frequency. We use 10k in this example. \n",
    "While the training job is running, we can use the <a href=\"http://localhost:6006\">tensorboard at port 6006</a> to monitor the training. Following are the training curves I have for training dataset and validation dataset.\n",
    "<!-- ![images/tensorboard_loss.png](images/tensorboard_loss.png) -->\n",
    "<center><img src=images/tensorboard_loss.png width=\"30%\" height=\"40%\" /></center>\n",
    "<center><strong>Figure:</strong> Example training and validation loss curves</center>\n",
    "</br>\n",
    "\n",
    "Clearly, the model is overfitted as shown in the validation curve. We can take the checkpoint file at step `76k` by modifying the `latest_checkpointed_iteration.txt` file to `76000` at the `CHECKPOINT_PATH`. Let's check the checkpoint file it generates\n",
    "\n",
    "### Adjust the model checkpoint to use the pretrained model we have provided for you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ebd88cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo 30000 > checkpoints/gpt_creditcard/latest_checkpointed_iteration.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec73eb64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000\n"
     ]
    }
   ],
   "source": [
    "!cat checkpoints/gpt_creditcard/latest_checkpointed_iteration.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1d3d67b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "# This example will start serving the model.\n",
      "source ./model_config.sh\n",
      "SEED=\"${1:-42}\"\n",
      "PORT=\"${2:-5000}\"\n",
      "echo $SEED\n",
      "\n",
      "DISTRIBUTED_ARGS=\"--nproc_per_node 1 \\\n",
      "                  --nnodes 1 \\\n",
      "                  --node_rank 0 \\\n",
      "                  --master_addr localhost \\\n",
      "                  --master_port 6000\"\n",
      "\n",
      "# CHECKPOINT=$LOADPATH\n",
      "VOCAB_FILE=credit_card_coder.pickle\n",
      "\n",
      "python -m torch.distributed.launch $DISTRIBUTED_ARGS tools/run_text_generation_server.py \\\n",
      "       --tensor-model-parallel-size $TENSOR_MP_SIZE  \\\n",
      "       --pipeline-model-parallel-size $PIPELINE_MP_SIZE  \\\n",
      "       --num-layers $NUM_LAYERS  \\\n",
      "       --hidden-size $HIDDEN_SIZE  \\\n",
      "       --load $CHECKPOINT_PATH  \\\n",
      "       --num-attention-heads $NUM_HEADS  \\\n",
      "       --max-position-embeddings $MAX_POS_EMD  \\\n",
      "       --tokenizer-type TabularTokenizer \\\n",
      "       --fp16  \\\n",
      "       --micro-batch-size 1  \\\n",
      "       --seq-length $SEQ_LEN  \\\n",
      "       --out-seq-length $SEQ_LEN  \\\n",
      "       --temperature 1.0  \\\n",
      "       --vocab-file $VOCAB_FILE  \\\n",
      "       --top_p 1.0  \\\n",
      "       --seed $SEED \\\n",
      "       --port $PORT\n"
     ]
    }
   ],
   "source": [
    "!cat ./run_data_gen_server.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e1e57c",
   "metadata": {},
   "source": [
    "Running the server will print out the incoming PUT requests. To silence these, comment out the `print` statements in `megatron/text_generation_server.py:MegatronGenerate:put` and re-run the script below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b77d0420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n",
      "/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated\n",
      "and will be removed in future. Use torchrun.\n",
      "Note that --use_env is set by default in torchrun.\n",
      "If your script expects `--local_rank` argument to be set, please\n",
      "change it to read from `os.environ['LOCAL_RANK']` instead. See \n",
      "https://pytorch.org/docs/stable/distributed.html#launch-utility for \n",
      "further instructions\n",
      "\n",
      "  warnings.warn(\n",
      "using world size: 1, data-parallel-size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "WARNING: overriding default arguments for tokenizer_type:TabularTokenizer                        with tokenizer_type:TabularTokenizer\n",
      "setting global batch size to 1\n",
      "using torch.float16 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  activations_checkpoint_method ................... None\n",
      "  activations_checkpoint_num_layers ............... 1\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_binary_head ................................ True\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... infer\n",
      "  data_parallel_size .............................. 1\n",
      "  data_path ....................................... None\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_seq_length .............................. None\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  encoder_seq_length .............................. 6144\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 100\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... None\n",
      "  ffn_hidden_size ................................. 4096\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ True\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  global_batch_size ............................... 1\n",
      "  greedy .......................................... False\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 1024\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_dim ......................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  kv_channels ..................................... 64\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ checkpoints/gpt_creditcard\n",
      "  local_rank ...................................... 0\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 100\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. None\n",
      "  lr_decay_iters .................................. None\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. linear\n",
      "  lr_warmup_fraction .............................. None\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 6144\n",
      "  merge_file ...................................... None\n",
      "  micro_batch_size ................................ 1\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 0.0\n",
      "  mmap_warmup ..................................... False\n",
      "  no_async_tensor_model_parallel_allreduce ........ False\n",
      "  no_load_optim ................................... True\n",
      "  no_load_rng ..................................... True\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  num_attention_heads ............................. 16\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_layers ...................................... 24\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  out_seq_length .................................. 6144\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float16\n",
      "  patch_dim ....................................... 16\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  pipeline_model_parallel_split_rank .............. None\n",
      "  port ............................................ 5000\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ None\n",
      "  save_interval ................................... None\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 42\n",
      "  seq_length ...................................... 6144\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 969, 30, 1\n",
      "  temperature ..................................... 1.0\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. None\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. TabularTokenizer\n",
      "  top_k ........................................... 0\n",
      "  top_p ........................................... 1.0\n",
      "  train_iters ..................................... None\n",
      "  train_samples ................................... None\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_contiguous_buffers_in_local_ddp ............. True\n",
      "  use_cpu_initialization .......................... None\n",
      "  use_one_sent_docs ............................... False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... credit_card_coder.pickle\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 1\n",
      "-------------------- end of arguments ---------------------\n",
      "setting number of micro-batches to constant 1\n",
      "> building TabularTokenizer tokenizer ...\n",
      " > padded vocab (size: 143695) with 49 dummy tokens (new size: 143744)\n",
      "> initializing torch distributed ...\n",
      "> initializing tensor model parallel with size 1\n",
      "> initializing pipeline model parallel with size 1\n",
      "> setting random seeds to 42 ...\n",
      "> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 2760 and data parallel seed: 42\n",
      "> compiling dataset index builder ...\n",
      "make: Entering directory '/sdg/host/megatron/data'\n",
      "make: Nothing to be done for 'default'.\n",
      "make: Leaving directory '/sdg/host/megatron/data'\n",
      ">>> done with dataset index builder. Compilation time: 0.043 seconds\n",
      "WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.\n",
      "> compiling and loading fused kernels ...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /sdg/host/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /sdg/host/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /sdg/host/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module fused_mix_prec_layer_norm_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_mix_prec_layer_norm_cuda...\n",
      ">>> done with compiling and loading fused kernels. Compilation time: 1.581 seconds\n",
      "building GPT model ...\n",
      " > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 455796736\n",
      " loading checkpoint from checkpoints/gpt_creditcard at iteration 30000\n",
      " checkpoint version 3.0\n",
      "  successfully loaded checkpoint from checkpoints/gpt_creditcard at iteration 30000\n",
      " * Serving Flask app 'megatron.text_generation_server' (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n",
      " * Running on all addresses.\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      " * Running on http://172.17.0.3:5000/ (Press CTRL+C to quit)\n",
      "request IP: 127.0.0.1\n",
      "current time:  2022-06-24 20:55:59.646814\n",
      "127.0.0.1 - - [24/Jun/2022 20:56:18] \"PUT /generate HTTP/1.1\" 200 -\n",
      "request IP: 127.0.0.1\n",
      "current time:  2022-06-24 20:56:41.766267\n",
      "127.0.0.1 - - [24/Jun/2022 20:56:45] \"PUT /generate HTTP/1.1\" 200 -\n",
      "request IP: 127.0.0.1\n",
      "current time:  2022-06-24 20:56:59.064505\n",
      "127.0.0.1 - - [24/Jun/2022 21:00:03] \"PUT /generate HTTP/1.1\" 200 -\n",
      "request IP: 127.0.0.1\n",
      "current time:  2022-06-24 21:16:50.466974\n",
      "127.0.0.1 - - [24/Jun/2022 21:19:30] \"PUT /generate HTTP/1.1\" 200 -\n",
      "request IP: 127.0.0.1\n",
      "current time:  2022-06-24 21:20:44.283161\n",
      "127.0.0.1 - - [24/Jun/2022 21:20:46] \"PUT /generate HTTP/1.1\" 200 -\n",
      "request IP: 127.0.0.1\n",
      "current time:  2022-06-24 21:20:46.291107\n",
      "127.0.0.1 - - [24/Jun/2022 21:20:48] \"PUT /generate HTTP/1.1\" 200 -\n",
      "request IP: 127.0.0.1\n",
      "current time:  2022-06-24 21:20:48.096992\n",
      "127.0.0.1 - - [24/Jun/2022 21:20:50] \"PUT /generate HTTP/1.1\" 200 -\n",
      "request IP: 127.0.0.1\n",
      "current time:  2022-06-24 21:20:50.032282\n",
      "127.0.0.1 - - [24/Jun/2022 21:20:51] \"PUT /generate HTTP/1.1\" 200 -\n",
      "request IP: 127.0.0.1\n",
      "current time:  2022-06-24 21:20:51.879098\n",
      "127.0.0.1 - - [24/Jun/2022 21:20:53] \"PUT /generate HTTP/1.1\" 200 -\n",
      "request IP: 127.0.0.1\n",
      "current time:  2022-06-24 21:20:53.678595\n",
      "127.0.0.1 - - [24/Jun/2022 21:20:55] \"PUT /generate HTTP/1.1\" 200 -\n",
      "request IP: 127.0.0.1\n",
      "current time:  2022-06-24 21:20:55.507690\n",
      "127.0.0.1 - - [24/Jun/2022 21:20:57] \"PUT /generate HTTP/1.1\" 200 -\n",
      "request IP: 127.0.0.1\n",
      "current time:  2022-06-24 21:20:57.330440\n",
      "127.0.0.1 - - [24/Jun/2022 21:20:59] \"PUT /generate HTTP/1.1\" 200 -\n",
      "request IP: 127.0.0.1\n",
      "current time:  2022-06-24 21:20:59.161448\n",
      "127.0.0.1 - - [24/Jun/2022 21:21:00] \"PUT /generate HTTP/1.1\" 200 -\n",
      "request IP: 127.0.0.1\n",
      "current time:  2022-06-24 21:21:00.970987\n",
      "127.0.0.1 - - [24/Jun/2022 21:21:02] \"PUT /generate HTTP/1.1\" 200 -\n",
      "request IP: 127.0.0.1\n",
      "current time:  2022-06-24 21:21:15.152190\n",
      "127.0.0.1 - - [24/Jun/2022 21:21:16] \"PUT /generate HTTP/1.1\" 200 -\n",
      "request IP: 127.0.0.1\n",
      "current time:  2022-06-24 21:21:16.983677\n",
      "127.0.0.1 - - [24/Jun/2022 21:21:18] \"PUT /generate HTTP/1.1\" 200 -\n",
      "request IP: 127.0.0.1\n",
      "current time:  2022-06-24 21:21:18.816113\n",
      "127.0.0.1 - - [24/Jun/2022 21:21:20] \"PUT /generate HTTP/1.1\" 200 -\n",
      "request IP: 127.0.0.1\n",
      "current time:  2022-06-24 21:21:20.648657\n",
      "127.0.0.1 - - [24/Jun/2022 21:21:22] \"PUT /generate HTTP/1.1\" 200 -\n",
      "request IP: 127.0.0.1\n",
      "current time:  2022-06-24 21:21:22.458612\n",
      "127.0.0.1 - - [24/Jun/2022 21:21:24] \"PUT /generate HTTP/1.1\" 200 -\n",
      "request IP: 127.0.0.1\n",
      "current time:  2022-06-24 21:21:24.285040\n",
      "127.0.0.1 - - [24/Jun/2022 21:21:26] \"PUT /generate HTTP/1.1\" 200 -\n",
      "request IP: 127.0.0.1\n",
      "current time:  2022-06-24 21:21:26.098493\n",
      "127.0.0.1 - - [24/Jun/2022 21:21:27] \"PUT /generate HTTP/1.1\" 200 -\n",
      "request IP: 127.0.0.1\n",
      "current time:  2022-06-24 21:21:27.900503\n",
      "127.0.0.1 - - [24/Jun/2022 21:21:29] \"PUT /generate HTTP/1.1\" 200 -\n",
      "request IP: 127.0.0.1\n",
      "current time:  2022-06-24 21:21:29.733053\n",
      "127.0.0.1 - - [24/Jun/2022 21:21:31] \"PUT /generate HTTP/1.1\" 200 -\n",
      "request IP: 127.0.0.1\n",
      "current time:  2022-06-24 21:21:31.588288\n",
      "127.0.0.1 - - [24/Jun/2022 21:21:33] \"PUT /generate HTTP/1.1\" 200 -\n",
      "request IP: 127.0.0.1\n",
      "current time:  2022-06-24 21:22:45.214029\n",
      "127.0.0.1 - - [24/Jun/2022 21:23:04] \"PUT /generate HTTP/1.1\" 200 -\n",
      "request IP: 127.0.0.1\n",
      "current time:  2022-06-24 21:23:07.928873\n",
      "127.0.0.1 - - [24/Jun/2022 21:23:10] \"PUT /generate HTTP/1.1\" 200 -\n",
      "request IP: 127.0.0.1\n",
      "current time:  2022-06-24 21:23:11.061246\n",
      "127.0.0.1 - - [24/Jun/2022 21:26:15] \"PUT /generate HTTP/1.1\" 200 -\n",
      "request IP: 127.0.0.1\n",
      "current time:  2022-06-24 21:26:15.097930\n",
      "127.0.0.1 - - [24/Jun/2022 21:26:16] \"PUT /generate HTTP/1.1\" 200 -\n",
      "request IP: 127.0.0.1\n",
      "current time:  2022-06-24 21:26:16.899097\n",
      "127.0.0.1 - - [24/Jun/2022 21:26:18] \"PUT /generate HTTP/1.1\" 200 -\n",
      "request IP: 127.0.0.1\n",
      "current time:  2022-06-24 21:26:18.695309\n",
      "127.0.0.1 - - [24/Jun/2022 21:26:20] \"PUT /generate HTTP/1.1\" 200 -\n",
      "request IP: 127.0.0.1\n",
      "current time:  2022-06-24 21:26:20.517125\n",
      "127.0.0.1 - - [24/Jun/2022 21:26:22] \"PUT /generate HTTP/1.1\" 200 -\n",
      "request IP: 127.0.0.1\n",
      "current time:  2022-06-24 21:26:22.319594\n",
      "127.0.0.1 - - [24/Jun/2022 21:26:24] \"PUT /generate HTTP/1.1\" 200 -\n",
      "request IP: 127.0.0.1\n",
      "current time:  2022-06-24 21:26:24.113216\n",
      "127.0.0.1 - - [24/Jun/2022 21:26:25] \"PUT /generate HTTP/1.1\" 200 -\n",
      "request IP: 127.0.0.1\n",
      "current time:  2022-06-24 21:26:25.893547\n",
      "127.0.0.1 - - [24/Jun/2022 21:26:27] \"PUT /generate HTTP/1.1\" 200 -\n",
      "request IP: 127.0.0.1\n",
      "current time:  2022-06-24 21:26:27.679677\n",
      "127.0.0.1 - - [24/Jun/2022 21:26:29] \"PUT /generate HTTP/1.1\" 200 -\n",
      "request IP: 127.0.0.1\n",
      "current time:  2022-06-24 21:26:29.495764\n",
      "127.0.0.1 - - [24/Jun/2022 21:26:31] \"PUT /generate HTTP/1.1\" 200 -\n",
      "request IP: 127.0.0.1\n",
      "current time:  2022-06-24 21:26:31.360535\n",
      "127.0.0.1 - - [24/Jun/2022 21:26:33] \"PUT /generate HTTP/1.1\" 200 -\n",
      "^C\n",
      "WARNING:torch.distributed.elastic.agent.server.api:Received 2 death signal, shutting down workers\n",
      "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 10844 closing signal SIGINT\n",
      "Traceback (most recent call last):\n",
      "  File \"tools/run_text_generation_server.py\", line 90, in <module>\n",
      "    torch.distributed.broadcast(choice, 0)\n",
      "  File \"/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py\", line 1188, in broadcast\n",
      "    work = default_pg.broadcast([tensor], opts)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!./run_data_gen_server.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5027bc11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
