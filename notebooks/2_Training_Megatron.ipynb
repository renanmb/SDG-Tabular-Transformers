{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "##\n",
    "## Copyright (C) 2022 NVIDIA Corporation.  All rights reserved.\n",
    "##\n",
    "## NVIDIA Sample Code\n",
    "##\n",
    "## Please refer to the NVIDIA end user license agreement (EULA) associated\n",
    "## with this source code for terms and conditions that govern your use of\n",
    "## this software. Any use, reproduction, disclosure, or distribution of\n",
    "## this software and related documentation outside the terms of the EULA\n",
    "## is strictly prohibited.\n",
    "##\n",
    "############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Megatron GPT Pretraining on Tabular Data\n",
    "\n",
    "\n",
    "### Tensor and Pipeline, and Data Parallelism in practice\n",
    "\n",
    "We are ready to pretrain the GPT model. As large models can be quite difficult to train due to memory constraints, Megatron makes it possible by using both Tensor parallelism and Pipeline parallelism that enables training transformer models with billions of parameters. Tensor parallelism and pipeline parallelism are orthogonal to each other. Recall the figure from the previous notebook that shows how to divide the large model horizontally (intra-layer) by tensor parallelism and vertically (across layers) by pipeline parallelism. Both of tensor and pipeline parallelism are types of <u>model parallelism</u>.\n",
    "\n",
    "<br>\n",
    "<center><img src=images/model_parallelism.png width=\"50%\" height=\"50%\" style=\"display=block; margin:auto\" alt=\"model parallelism\"/></center>\n",
    "<br>\n",
    "\n",
    "In addition to model parallelism, we can apply <u>data parallelism</u> to the training to fully utilize all the GPUs in the cluster. This [paper](https://arxiv.org/pdf/2104.04473.pdf) provides a few takeaways about how to optimally setup the model parallelism and data parallelism:\n",
    "\n",
    "1. When considering different forms of model parallelism, tensor model parallelism should generally be used up to degree ùëî when using ùëî-GPU servers, and then pipeline model parallelism can be used to scale up to larger models across server.\n",
    "2. When using data and model parallelism, a total model-parallel size of ùëÄ = ùë° ¬∑ ùëù should be used so that the model‚Äôs parameters and intermediate metadata fit in GPU memory; data parallelism can be used to scale up training to more GPUs. <br>In this case, ùë° is the tensor parallel size, and ùëù is the pipeline parallel size.\n",
    "3. The optimal micro batch size ùëè depends on the throughput and memory footprint characteristics of the model, as well as the pipeline depth ùëù, data-parallel size ùëë, and batch size ùêµ.\n",
    "\n",
    "In our experiment, we are only concerned with training a model that fits into a single GPU. we set the tensor model parallel and pipeline model parallelism parameter to 1. Here is the script we used for the pretraining task.\n",
    "\n",
    "```bash\n",
    "#! /bin/bash\n",
    "source ./model_config.sh\n",
    "\n",
    "python -m torch.distributed.launch $DISTRIBUTED_ARGS \\\n",
    "       pretrain_gpt.py \\\n",
    "       --num-layers $NUM_LAYERS \\\n",
    "       --hidden-size $HIDDEN_SIZE \\\n",
    "       --num-attention-heads $NUM_HEADS \\\n",
    "       --micro-batch-size 4 \\\n",
    "       --global-batch-size 32 \\\n",
    "       --seq-length $SEQ_LEN \\\n",
    "       --max-position-embeddings $MAX_POS_EMD \\\n",
    "       --train-iters 500000 \\\n",
    "       --lr-decay-iters 320000 \\\n",
    "       --tensorboard-dir $TB_PATH \\\n",
    "       --save $CHECKPOINT_PATH \\\n",
    "       --data-path $DATA_PATH \\\n",
    "       --data-impl mmap \\\n",
    "       --split 949,50,1 \\\n",
    "       --distributed-backend nccl \\\n",
    "       --tensor-model-parallel-size $TENSOR_MP_SIZE \\\n",
    "       --pipeline-model-parallel-size $PIPELINE_MP_SIZE \\\n",
    "       --lr 0.00015 \\\n",
    "       --lr-decay-style cosine \\\n",
    "       --min-lr 1.0e-5 \\\n",
    "       --weight-decay 1e-2 \\\n",
    "       --clip-grad 1.0 \\\n",
    "       --lr-warmup-fraction .01 \\\n",
    "       --checkpoint-activations \\\n",
    "       --log-interval 100 \\\n",
    "       --save-interval 5000 \\\n",
    "       --eval-interval 1000 \\\n",
    "       --eval-iters 10 \\\n",
    "       --load $LOADPATH \\\n",
    "       --vocab-file $VOCAB_FILE \\\n",
    "       --fp16\n",
    "```\n",
    "\n",
    "Run the pretraining task script below. Note, this is the most time consuming step. It can take days for the task to converge depending on the computation environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # IF YOU ARE RETRAINING, YOU MAY WANT TO DELETE ALL THE PREVIOUS MODEL CHECKPOINTS\n",
    "# OUTPUT_PATH='checkpoints'\n",
    "# import os\n",
    "# import shutil\n",
    "# if os.path.isdir(OUTPUT_PATH):\n",
    "#     shutil.rmtree(OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Mar 26 03:35:36 UTC 2022\n"
     ]
    }
   ],
   "source": [
    "!date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While running the cell below, the model checkpoints and tensorboard events will be saved to:\n",
    "\n",
    "```\n",
    "TOY_MODEL_CHECKPOINT_PATH=checkpoints/gpt_toy_model\n",
    "TOY_MODEL_TB_PATH=checkpoints/checkpoints/tb/toy_model\n",
    "```\n",
    "as defined in the <a href=\"./model_config.sh\">model_config.sh</a> script and could be used for viewing the Tensorboard or for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:178: FutureWarning: The module torch.distributed.launch is deprecated\n",
      "and will be removed in future. Use torchrun.\n",
      "Note that --use_env is set by default in torchrun.\n",
      "If your script expects `--local_rank` argument to be set, please\n",
      "change it to read from `os.environ['LOCAL_RANK']` instead. See \n",
      "https://pytorch.org/docs/stable/distributed.html#launch-utility for \n",
      "further instructions\n",
      "\n",
      "  warnings.warn(\n",
      "WARNING:torch.distributed.run:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "using world size: 8, data-parallel-size: 8, tensor-model-parallel size: 1, pipeline-model-parallel size: 1 \n",
      "--checkpoint-activations is no longer valid, use --activation-checkpoint-method instead. Defaulting to activation-checkpoint-method=uniform.\n",
      "using torch.float16 for parameters ...\n",
      "------------------------ arguments ------------------------\n",
      "  accumulate_allreduce_grads_in_fp32 .............. False\n",
      "  activations_checkpoint_method ................... uniform\n",
      "  activations_checkpoint_num_layers ............... 1\n",
      "  adam_beta1 ...................................... 0.9\n",
      "  adam_beta2 ...................................... 0.999\n",
      "  adam_eps ........................................ 1e-08\n",
      "  adlr_autoresume ................................. False\n",
      "  adlr_autoresume_interval ........................ 1000\n",
      "  apply_query_key_layer_scaling ................... True\n",
      "  apply_residual_connection_post_layernorm ........ False\n",
      "  attention_dropout ............................... 0.1\n",
      "  attention_softmax_in_fp32 ....................... False\n",
      "  bert_binary_head ................................ True\n",
      "  bert_load ....................................... None\n",
      "  bf16 ............................................ False\n",
      "  bias_dropout_fusion ............................. True\n",
      "  bias_gelu_fusion ................................ True\n",
      "  biencoder_projection_dim ........................ 0\n",
      "  biencoder_shared_query_context_model ............ False\n",
      "  block_data_path ................................. None\n",
      "  clip_grad ....................................... 1.0\n",
      "  consumed_train_samples .......................... 0\n",
      "  consumed_valid_samples .......................... 0\n",
      "  data_impl ....................................... mmap\n",
      "  data_parallel_size .............................. 8\n",
      "  data_path ....................................... ['creditcard_text_document']\n",
      "  dataloader_type ................................. single\n",
      "  DDP_impl ........................................ local\n",
      "  decoder_seq_length .............................. None\n",
      "  distribute_checkpointed_activations ............. False\n",
      "  distributed_backend ............................. nccl\n",
      "  embedding_path .................................. None\n",
      "  empty_unused_memory_level ....................... 0\n",
      "  encoder_seq_length .............................. 6144\n",
      "  eod_mask_loss ................................... False\n",
      "  eval_interval ................................... 1000\n",
      "  eval_iters ...................................... 10\n",
      "  evidence_data_path .............................. None\n",
      "  exit_duration_in_mins ........................... None\n",
      "  exit_interval ................................... None\n",
      "  ffn_hidden_size ................................. 4096\n",
      "  finetune ........................................ False\n",
      "  fp16 ............................................ True\n",
      "  fp16_lm_cross_entropy ........................... False\n",
      "  fp32_residual_connection ........................ False\n",
      "  global_batch_size ............................... 8\n",
      "  hidden_dropout .................................. 0.1\n",
      "  hidden_size ..................................... 1024\n",
      "  hysteresis ...................................... 2\n",
      "  ict_head_size ................................... None\n",
      "  ict_load ........................................ None\n",
      "  img_dim ......................................... 224\n",
      "  indexer_batch_size .............................. 128\n",
      "  indexer_log_interval ............................ 1000\n",
      "  init_method_std ................................. 0.02\n",
      "  init_method_xavier_uniform ...................... False\n",
      "  initial_loss_scale .............................. 4294967296\n",
      "  kv_channels ..................................... 64\n",
      "  layernorm_epsilon ............................... 1e-05\n",
      "  lazy_mpu_init ................................... None\n",
      "  load ............................................ checkpoints/gpt_creditcard\n",
      "  local_rank ...................................... 0\n",
      "  log_batch_size_to_tensorboard ................... False\n",
      "  log_interval .................................... 500\n",
      "  log_learning_rate_to_tensorboard ................ True\n",
      "  log_loss_scale_to_tensorboard ................... True\n",
      "  log_memory_to_tensorboard ....................... False\n",
      "  log_num_zeros_in_grad ........................... False\n",
      "  log_params_norm ................................. False\n",
      "  log_timers_to_tensorboard ....................... False\n",
      "  log_validation_ppl_to_tensorboard ............... False\n",
      "  loss_scale ...................................... None\n",
      "  loss_scale_window ............................... 1000\n",
      "  lr .............................................. 0.00015\n",
      "  lr_decay_iters .................................. 320000\n",
      "  lr_decay_samples ................................ None\n",
      "  lr_decay_style .................................. cosine\n",
      "  lr_warmup_fraction .............................. 0.01\n",
      "  lr_warmup_iters ................................. 0\n",
      "  lr_warmup_samples ............................... 0\n",
      "  make_vocab_size_divisible_by .................... 128\n",
      "  mask_prob ....................................... 0.15\n",
      "  masked_softmax_fusion ........................... True\n",
      "  max_position_embeddings ......................... 6144\n",
      "  merge_file ...................................... None\n",
      "  micro_batch_size ................................ 1\n",
      "  min_loss_scale .................................. 1.0\n",
      "  min_lr .......................................... 1e-05\n",
      "  mmap_warmup ..................................... False\n",
      "  no_async_tensor_model_parallel_allreduce ........ False\n",
      "  no_load_optim ................................... None\n",
      "  no_load_rng ..................................... None\n",
      "  no_save_optim ................................... None\n",
      "  no_save_rng ..................................... None\n",
      "  num_attention_heads ............................. 16\n",
      "  num_channels .................................... 3\n",
      "  num_classes ..................................... 1000\n",
      "  num_layers ...................................... 24\n",
      "  num_layers_per_virtual_pipeline_stage ........... None\n",
      "  num_workers ..................................... 2\n",
      "  onnx_safe ....................................... None\n",
      "  openai_gelu ..................................... False\n",
      "  optimizer ....................................... adam\n",
      "  override_lr_scheduler ........................... False\n",
      "  params_dtype .................................... torch.float16\n",
      "  patch_dim ....................................... 16\n",
      "  pipeline_model_parallel_size .................... 1\n",
      "  pipeline_model_parallel_split_rank .............. None\n",
      "  query_in_block_prob ............................. 0.1\n",
      "  rampup_batch_size ............................... None\n",
      "  rank ............................................ 0\n",
      "  reset_attention_mask ............................ False\n",
      "  reset_position_ids .............................. False\n",
      "  retriever_report_topk_accuracies ................ []\n",
      "  retriever_score_scaling ......................... False\n",
      "  retriever_seq_length ............................ 256\n",
      "  sample_rate ..................................... 1.0\n",
      "  save ............................................ checkpoints/gpt_creditcard\n",
      "  save_interval ................................... 5000\n",
      "  scatter_gather_tensors_in_pipeline .............. True\n",
      "  seed ............................................ 1234\n",
      "  seq_length ...................................... 6144\n",
      "  sgd_momentum .................................... 0.9\n",
      "  short_seq_prob .................................. 0.1\n",
      "  split ........................................... 949,50,1\n",
      "  tensor_model_parallel_size ...................... 1\n",
      "  tensorboard_dir ................................. checkpoints/checkpoints/tb\n",
      "  tensorboard_log_interval ........................ 1\n",
      "  tensorboard_queue_size .......................... 1000\n",
      "  titles_data_path ................................ None\n",
      "  tokenizer_type .................................. TabularTokenizer\n",
      "  train_iters ..................................... 125000\n",
      "  train_samples ................................... None\n",
      "  use_checkpoint_lr_scheduler ..................... False\n",
      "  use_contiguous_buffers_in_local_ddp ............. True\n",
      "  use_cpu_initialization .......................... None\n",
      "  use_one_sent_docs ............................... False\n",
      "  virtual_pipeline_model_parallel_size ............ None\n",
      "  vocab_extra_ids ................................. 0\n",
      "  vocab_file ...................................... credit_card_coder.pickle\n",
      "  weight_decay .................................... 0.01\n",
      "  world_size ...................................... 8\n",
      "-------------------- end of arguments ---------------------\n",
      "setting number of micro-batches to constant 1\n",
      "> building TabularTokenizer tokenizer ...\n",
      " > padded vocab (size: 143695) with 49 dummy tokens (new size: 143744)\n",
      "> initializing torch distributed ...\n",
      "> setting tensorboard ...\n",
      "> initializing tensor model parallel with size 1\n",
      "> initializing pipeline model parallel with size 1\n",
      "> setting random seeds to 1234 ...\n",
      "> initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234\n",
      "> compiling dataset index builder ...\n",
      "make: Entering directory '/workspace/megatron/data'\n",
      "make: Nothing to be done for 'default'.\n",
      "make: Leaving directory '/workspace/megatron/data'\n",
      ">>> done with dataset index builder. Compilation time: 0.063 seconds\n",
      "WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.\n",
      "> compiling and loading fused kernels ...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /workspace/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /workspace/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /workspace/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module fused_mix_prec_layer_norm_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_mix_prec_layer_norm_cuda...\n",
      ">>> done with compiling and loading fused kernels. Compilation time: 6.975 seconds\n",
      "time to initialize megatron (seconds): 7.615\n",
      "[after megatron is initialized] datetime: 2022-03-26 03:42:06 \n",
      "building GPT model ...\n",
      " > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 455796736\n",
      "> learning rate decay style: cosine\n",
      "WARNING: could not find the metadata file checkpoints/gpt_creditcard/latest_checkpointed_iteration.txt \n",
      "    will not load any checkpoints and will start from random\n",
      "time (ms) | load-checkpoint: 0.28\n",
      "[after model, optimizer, and learning rate scheduler are built] datetime: 2022-03-26 03:42:07 \n",
      "> building train, validation, and test datasets ...\n",
      " > datasets target sizes (minimum size):\n",
      "    train:      1000000\n",
      "    validation: 10080\n",
      "    test:       80\n",
      "> building train, validation, and test datasets for GPT ...\n",
      " > building dataset index ...\n",
      "    reading sizes...\n",
      "    reading pointers...\n",
      "    reading document index...\n",
      "    creating numpy buffer of mmap...\n",
      "    creating memory view of numpy buffer...\n",
      " > finished creating indexed dataset in 0.000667 seconds\n",
      "    number of documents: 98574\n",
      " > dataset split:\n",
      "    train:\n",
      "     document indices in [0, 93546) total of 93546 documents\n",
      "    validation:\n",
      "     document indices in [93546, 98475) total of 4929 documents\n",
      "    test:\n",
      "     document indices in [98475, 98574) total of 99 documents\n",
      " > loading doc-idx mapping from creditcard_text_document_train_indexmap_1000000ns_6144sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from creditcard_text_document_train_indexmap_1000000ns_6144sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from creditcard_text_document_train_indexmap_1000000ns_6144sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.002 seconds\n",
      "    total number of samples: 1074897\n",
      "    total number of epochs: 6\n",
      " > loading doc-idx mapping from creditcard_text_document_valid_indexmap_10080ns_6144sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from creditcard_text_document_valid_indexmap_10080ns_6144sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from creditcard_text_document_valid_indexmap_10080ns_6144sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 18867\n",
      "    total number of epochs: 2\n",
      " > loading doc-idx mapping from creditcard_text_document_test_indexmap_80ns_6144sl_1234s_doc_idx.npy\n",
      " > loading sample-idx mapping from creditcard_text_document_test_indexmap_80ns_6144sl_1234s_sample_idx.npy\n",
      " > loading shuffle-idx mapping from creditcard_text_document_test_indexmap_80ns_6144sl_1234s_shuffle_idx.npy\n",
      "    loaded indexed file in 0.001 seconds\n",
      "    total number of samples: 186\n",
      "    total number of epochs: 1\n",
      "> finished creating GPT datasets ...\n",
      "[after dataloaders are built] datetime: 2022-03-26 03:42:09 \n",
      "done with setup ...\n",
      "training ...\n",
      "time (ms) | model-and-optimizer-setup: 389.63 | train/valid/test-data-iterators-setup: 2270.70\n",
      "[before the start of training step] datetime: 2022-03-26 03:42:09 \n",
      " iteration      500/  125000 | consumed samples:         4000 | elapsed time per iteration (ms): 3994.3 | learning rate: 2.264E-05 | global batch size:     8 | lm loss: 5.557446E+00 | loss scale: 65536.0 | grad norm: 3.779 | number of skipped iterations:  17 | number of nan iterations:   0 |\n",
      "[Rank 0] (after 500 iterations) memory (MB) | allocated: 8694.7744140625 | max allocated: 16878.94091796875 | reserved: 21012.0 | max reserved: 21012.0\n",
      "time (ms) | forward-compute: 1153.00 | backward-compute: 2772.58 | backward-params-all-reduce: 22.44 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 5.65 | optimizer-clip-main-grad: 8.69 | optimizer-copy-main-to-model-params: 5.58 | optimizer: 42.58 | batch-generator: 2.24\n",
      " iteration     1000/  125000 | consumed samples:         8000 | elapsed time per iteration (ms): 3991.3 | learning rate: 4.608E-05 | global batch size:     8 | lm loss: 2.620539E+00 | loss scale: 65536.0 | grad norm: 1.304 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1149.95 | backward-compute: 2771.64 | backward-params-all-reduce: 22.74 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.17 | optimizer-unscale-and-check-inf: 5.37 | optimizer-clip-main-grad: 9.00 | optimizer-copy-main-to-model-params: 5.63 | optimizer: 43.25 | batch-generator: 2.17\n",
      "------------------------------------------------------------------------------------------------\n",
      " validation loss at iteration 1000 | lm loss value: 2.947755E+00 | lm loss PPL: 1.906310E+01 | \n",
      "------------------------------------------------------------------------------------------------\n",
      " iteration     1500/  125000 | consumed samples:        12000 | elapsed time per iteration (ms): 4011.9 | learning rate: 6.952E-05 | global batch size:     8 | lm loss: 2.038610E+00 | loss scale: 131072.0 | grad norm: 1.128 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1171.10 | backward-compute: 2771.39 | backward-params-all-reduce: 22.48 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.17 | optimizer-unscale-and-check-inf: 5.35 | optimizer-clip-main-grad: 8.86 | optimizer-copy-main-to-model-params: 5.75 | optimizer: 43.23 | batch-generator: 2.27\n",
      " iteration     2000/  125000 | consumed samples:        16000 | elapsed time per iteration (ms): 3987.4 | learning rate: 9.286E-05 | global batch size:     8 | lm loss: 1.544210E+00 | loss scale: 65536.0 | grad norm: 0.678 | number of skipped iterations:   2 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1150.84 | backward-compute: 2771.02 | backward-params-all-reduce: 22.74 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.17 | optimizer-unscale-and-check-inf: 5.35 | optimizer-clip-main-grad: 4.79 | optimizer-copy-main-to-model-params: 5.71 | optimizer: 39.04 | batch-generator: 2.26\n",
      "------------------------------------------------------------------------------------------------\n",
      " validation loss at iteration 2000 | lm loss value: 2.342385E+00 | lm loss PPL: 1.040603E+01 | \n",
      "------------------------------------------------------------------------------------------------\n",
      " iteration     2500/  125000 | consumed samples:        20000 | elapsed time per iteration (ms): 4006.3 | learning rate: 1.163E-04 | global batch size:     8 | lm loss: 1.239608E+00 | loss scale: 65536.0 | grad norm: 0.637 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1171.08 | backward-compute: 2770.52 | backward-params-all-reduce: 22.51 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 5.34 | optimizer-clip-main-grad: 4.26 | optimizer-copy-main-to-model-params: 5.62 | optimizer: 38.47 | batch-generator: 2.30\n",
      " iteration     3000/  125000 | consumed samples:        24000 | elapsed time per iteration (ms): 3985.5 | learning rate: 1.397E-04 | global batch size:     8 | lm loss: 1.020325E+00 | loss scale: 131072.0 | grad norm: 0.486 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1150.14 | backward-compute: 2770.36 | backward-params-all-reduce: 22.71 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 5.34 | optimizer-clip-main-grad: 4.26 | optimizer-copy-main-to-model-params: 5.73 | optimizer: 38.57 | batch-generator: 2.23\n",
      "------------------------------------------------------------------------------------------------\n",
      " validation loss at iteration 3000 | lm loss value: 1.757052E+00 | lm loss PPL: 5.795329E+00 | \n",
      "------------------------------------------------------------------------------------------------\n",
      " iteration     3500/  125000 | consumed samples:        28000 | elapsed time per iteration (ms): 4006.0 | learning rate: 1.500E-04 | global batch size:     8 | lm loss: 8.949254E-01 | loss scale: 131072.0 | grad norm: 0.378 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1170.98 | backward-compute: 2770.12 | backward-params-all-reduce: 22.63 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 5.34 | optimizer-clip-main-grad: 4.25 | optimizer-copy-main-to-model-params: 5.73 | optimizer: 38.58 | batch-generator: 2.30\n",
      " iteration     4000/  125000 | consumed samples:        32000 | elapsed time per iteration (ms): 3984.3 | learning rate: 1.500E-04 | global batch size:     8 | lm loss: 8.245705E-01 | loss scale: 262144.0 | grad norm: 0.310 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1149.42 | backward-compute: 2770.09 | backward-params-all-reduce: 22.62 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 5.35 | optimizer-clip-main-grad: 4.25 | optimizer-copy-main-to-model-params: 5.62 | optimizer: 38.47 | batch-generator: 2.29\n",
      "------------------------------------------------------------------------------------------------\n",
      " validation loss at iteration 4000 | lm loss value: 1.470155E+00 | lm loss PPL: 4.349911E+00 | \n",
      "------------------------------------------------------------------------------------------------\n",
      " iteration     4500/  125000 | consumed samples:        36000 | elapsed time per iteration (ms): 4005.2 | learning rate: 1.500E-04 | global batch size:     8 | lm loss: 7.892431E-01 | loss scale: 65536.0 | grad norm: 0.263 | number of skipped iterations:   3 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1170.82 | backward-compute: 2769.84 | backward-params-all-reduce: 22.40 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 5.35 | optimizer-clip-main-grad: 4.28 | optimizer-copy-main-to-model-params: 5.70 | optimizer: 38.47 | batch-generator: 2.31\n",
      " iteration     5000/  125000 | consumed samples:        40000 | elapsed time per iteration (ms): 3983.8 | learning rate: 1.500E-04 | global batch size:     8 | lm loss: 7.661527E-01 | loss scale: 32768.0 | grad norm: 0.309 | number of skipped iterations:   1 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1149.66 | backward-compute: 2769.55 | backward-params-all-reduce: 22.30 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 5.35 | optimizer-clip-main-grad: 4.30 | optimizer-copy-main-to-model-params: 5.72 | optimizer: 38.58 | batch-generator: 2.22\n",
      "------------------------------------------------------------------------------------------------\n",
      " validation loss at iteration 5000 | lm loss value: 1.462972E+00 | lm loss PPL: 4.318774E+00 | \n",
      "------------------------------------------------------------------------------------------------\n",
      "saving checkpoint at iteration    5000 to checkpoints/gpt_creditcard\n",
      "  successfully saved checkpoint at iteration    5000 to checkpoints/gpt_creditcard\n",
      "time (ms) | save-checkpoint: 11365.79\n",
      " iteration     5500/  125000 | consumed samples:        44000 | elapsed time per iteration (ms): 4027.7 | learning rate: 1.500E-04 | global batch size:     8 | lm loss: 7.496115E-01 | loss scale: 32768.0 | grad norm: 0.222 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1171.08 | backward-compute: 2769.30 | backward-params-all-reduce: 22.35 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.17 | optimizer-unscale-and-check-inf: 5.35 | optimizer-clip-main-grad: 4.26 | optimizer-copy-main-to-model-params: 5.62 | optimizer: 38.49 | batch-generator: 2.27\n",
      " iteration     6000/  125000 | consumed samples:        48000 | elapsed time per iteration (ms): 3984.6 | learning rate: 1.500E-04 | global batch size:     8 | lm loss: 7.345318E-01 | loss scale: 65536.0 | grad norm: 0.225 | number of skipped iterations:   1 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1150.57 | backward-compute: 2769.51 | backward-params-all-reduce: 22.37 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 5.34 | optimizer-clip-main-grad: 4.22 | optimizer-copy-main-to-model-params: 5.72 | optimizer: 38.48 | batch-generator: 2.31\n",
      "------------------------------------------------------------------------------------------------\n",
      " validation loss at iteration 6000 | lm loss value: 1.432454E+00 | lm loss PPL: 4.188965E+00 | \n",
      "------------------------------------------------------------------------------------------------\n",
      " iteration     6500/  125000 | consumed samples:        52000 | elapsed time per iteration (ms): 4005.6 | learning rate: 1.500E-04 | global batch size:     8 | lm loss: 7.290205E-01 | loss scale: 32768.0 | grad norm: 0.241 | number of skipped iterations:   1 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1171.46 | backward-compute: 2769.44 | backward-params-all-reduce: 22.42 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 5.35 | optimizer-clip-main-grad: 4.25 | optimizer-copy-main-to-model-params: 5.72 | optimizer: 38.53 | batch-generator: 2.28\n",
      " iteration     7000/  125000 | consumed samples:        56000 | elapsed time per iteration (ms): 3983.8 | learning rate: 1.500E-04 | global batch size:     8 | lm loss: 7.208239E-01 | loss scale: 32768.0 | grad norm: 0.219 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1149.86 | backward-compute: 2769.25 | backward-params-all-reduce: 22.39 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.17 | optimizer-unscale-and-check-inf: 5.34 | optimizer-clip-main-grad: 4.26 | optimizer-copy-main-to-model-params: 5.75 | optimizer: 38.58 | batch-generator: 2.16\n",
      "------------------------------------------------------------------------------------------------\n",
      " validation loss at iteration 7000 | lm loss value: 1.478701E+00 | lm loss PPL: 4.387242E+00 | \n",
      "------------------------------------------------------------------------------------------------\n",
      " iteration     7500/  125000 | consumed samples:        60000 | elapsed time per iteration (ms): 4004.7 | learning rate: 1.499E-04 | global batch size:     8 | lm loss: 7.189896E-01 | loss scale: 32768.0 | grad norm: 0.262 | number of skipped iterations:   2 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1170.90 | backward-compute: 2769.23 | backward-params-all-reduce: 22.41 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 5.35 | optimizer-clip-main-grad: 4.30 | optimizer-copy-main-to-model-params: 5.59 | optimizer: 38.41 | batch-generator: 2.26\n",
      " iteration     8000/  125000 | consumed samples:        64000 | elapsed time per iteration (ms): 3983.5 | learning rate: 1.499E-04 | global batch size:     8 | lm loss: 7.121484E-01 | loss scale: 32768.0 | grad norm: 0.707 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1149.69 | backward-compute: 2769.23 | backward-params-all-reduce: 22.30 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 5.35 | optimizer-clip-main-grad: 4.28 | optimizer-copy-main-to-model-params: 5.73 | optimizer: 38.61 | batch-generator: 2.22\n",
      "------------------------------------------------------------------------------------------------\n",
      " validation loss at iteration 8000 | lm loss value: 1.434873E+00 | lm loss PPL: 4.199113E+00 | \n",
      "------------------------------------------------------------------------------------------------\n",
      " iteration     8500/  125000 | consumed samples:        68000 | elapsed time per iteration (ms): 4004.9 | learning rate: 1.499E-04 | global batch size:     8 | lm loss: 7.048030E-01 | loss scale: 65536.0 | grad norm: 0.236 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1171.04 | backward-compute: 2769.24 | backward-params-all-reduce: 22.39 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 5.34 | optimizer-clip-main-grad: 4.24 | optimizer-copy-main-to-model-params: 5.73 | optimizer: 38.56 | batch-generator: 2.36\n",
      " iteration     9000/  125000 | consumed samples:        72000 | elapsed time per iteration (ms): 3983.9 | learning rate: 1.499E-04 | global batch size:     8 | lm loss: 7.037111E-01 | loss scale: 65536.0 | grad norm: 0.313 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1149.81 | backward-compute: 2769.40 | backward-params-all-reduce: 22.43 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 5.35 | optimizer-clip-main-grad: 4.28 | optimizer-copy-main-to-model-params: 5.62 | optimizer: 38.50 | batch-generator: 2.31\n",
      "------------------------------------------------------------------------------------------------\n",
      " validation loss at iteration 9000 | lm loss value: 1.407705E+00 | lm loss PPL: 4.086565E+00 | \n",
      "------------------------------------------------------------------------------------------------\n",
      " iteration     9500/  125000 | consumed samples:        76000 | elapsed time per iteration (ms): 4005.5 | learning rate: 1.499E-04 | global batch size:     8 | lm loss: 6.995980E-01 | loss scale: 131072.0 | grad norm: 0.196 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1171.17 | backward-compute: 2769.41 | backward-params-all-reduce: 22.54 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 5.36 | optimizer-clip-main-grad: 4.27 | optimizer-copy-main-to-model-params: 5.75 | optimizer: 38.63 | batch-generator: 2.35\n",
      " iteration    10000/  125000 | consumed samples:        80000 | elapsed time per iteration (ms): 3984.3 | learning rate: 1.498E-04 | global batch size:     8 | lm loss: 6.947853E-01 | loss scale: 131072.0 | grad norm: 0.172 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1149.87 | backward-compute: 2769.66 | backward-params-all-reduce: 22.45 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 5.35 | optimizer-clip-main-grad: 4.25 | optimizer-copy-main-to-model-params: 5.75 | optimizer: 38.60 | batch-generator: 2.32\n",
      "-------------------------------------------------------------------------------------------------\n",
      " validation loss at iteration 10000 | lm loss value: 1.452774E+00 | lm loss PPL: 4.274957E+00 | \n",
      "-------------------------------------------------------------------------------------------------\n",
      "saving checkpoint at iteration   10000 to checkpoints/gpt_creditcard\n",
      "  successfully saved checkpoint at iteration   10000 to checkpoints/gpt_creditcard\n",
      "time (ms) | save-checkpoint: 11452.00\n",
      " iteration    10500/  125000 | consumed samples:        84000 | elapsed time per iteration (ms): 4027.9 | learning rate: 1.498E-04 | global batch size:     8 | lm loss: 6.928697E-01 | loss scale: 65536.0 | grad norm: 0.229 | number of skipped iterations:   2 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1170.94 | backward-compute: 2769.64 | backward-params-all-reduce: 22.38 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 5.34 | optimizer-clip-main-grad: 4.23 | optimizer-copy-main-to-model-params: 5.59 | optimizer: 38.34 | batch-generator: 2.31\n",
      " iteration    11000/  125000 | consumed samples:        88000 | elapsed time per iteration (ms): 3983.8 | learning rate: 1.498E-04 | global batch size:     8 | lm loss: 6.896254E-01 | loss scale: 65536.0 | grad norm: 0.165 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1149.54 | backward-compute: 2769.42 | backward-params-all-reduce: 22.44 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.17 | optimizer-unscale-and-check-inf: 5.35 | optimizer-clip-main-grad: 4.26 | optimizer-copy-main-to-model-params: 5.76 | optimizer: 38.62 | batch-generator: 2.22\n",
      "-------------------------------------------------------------------------------------------------\n",
      " validation loss at iteration 11000 | lm loss value: 1.513444E+00 | lm loss PPL: 4.542347E+00 | \n",
      "-------------------------------------------------------------------------------------------------\n",
      " iteration    11500/  125000 | consumed samples:        92000 | elapsed time per iteration (ms): 4005.0 | learning rate: 1.498E-04 | global batch size:     8 | lm loss: 6.857015E-01 | loss scale: 131072.0 | grad norm: 0.199 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1170.91 | backward-compute: 2769.42 | backward-params-all-reduce: 22.44 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 5.34 | optimizer-clip-main-grad: 4.24 | optimizer-copy-main-to-model-params: 5.73 | optimizer: 38.56 | batch-generator: 2.29\n",
      " iteration    12000/  125000 | consumed samples:        96000 | elapsed time per iteration (ms): 3984.3 | learning rate: 1.497E-04 | global batch size:     8 | lm loss: 6.861882E-01 | loss scale: 131072.0 | grad norm: 0.170 | number of skipped iterations:   1 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1150.06 | backward-compute: 2769.70 | backward-params-all-reduce: 22.50 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 5.34 | optimizer-clip-main-grad: 4.22 | optimizer-copy-main-to-model-params: 5.60 | optimizer: 38.35 | batch-generator: 2.26\n",
      "-------------------------------------------------------------------------------------------------\n",
      " validation loss at iteration 12000 | lm loss value: 1.380139E+00 | lm loss PPL: 3.975453E+00 | \n",
      "-------------------------------------------------------------------------------------------------\n",
      " iteration    12500/  125000 | consumed samples:       100000 | elapsed time per iteration (ms): 4005.0 | learning rate: 1.497E-04 | global batch size:     8 | lm loss: 6.859535E-01 | loss scale: 65536.0 | grad norm: 0.179 | number of skipped iterations:   1 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1170.74 | backward-compute: 2769.55 | backward-params-all-reduce: 22.46 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 5.34 | optimizer-clip-main-grad: 4.25 | optimizer-copy-main-to-model-params: 5.72 | optimizer: 38.51 | batch-generator: 2.22\n",
      " iteration    13000/  125000 | consumed samples:       104000 | elapsed time per iteration (ms): 3984.5 | learning rate: 1.497E-04 | global batch size:     8 | lm loss: 6.792830E-01 | loss scale: 65536.0 | grad norm: 0.151 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1150.40 | backward-compute: 2769.34 | backward-params-all-reduce: 22.52 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 5.33 | optimizer-clip-main-grad: 4.22 | optimizer-copy-main-to-model-params: 5.73 | optimizer: 38.53 | batch-generator: 2.22\n",
      "-------------------------------------------------------------------------------------------------\n",
      " validation loss at iteration 13000 | lm loss value: 1.346638E+00 | lm loss PPL: 3.844478E+00 | \n",
      "-------------------------------------------------------------------------------------------------\n",
      " iteration    13500/  125000 | consumed samples:       108000 | elapsed time per iteration (ms): 4005.2 | learning rate: 1.496E-04 | global batch size:     8 | lm loss: 6.771658E-01 | loss scale: 131072.0 | grad norm: 0.157 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1171.05 | backward-compute: 2769.56 | backward-params-all-reduce: 22.41 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 5.33 | optimizer-clip-main-grad: 4.22 | optimizer-copy-main-to-model-params: 5.62 | optimizer: 38.41 | batch-generator: 2.29\n",
      " iteration    14000/  125000 | consumed samples:       112000 | elapsed time per iteration (ms): 3984.4 | learning rate: 1.496E-04 | global batch size:     8 | lm loss: 6.750950E-01 | loss scale: 131072.0 | grad norm: 0.163 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1149.99 | backward-compute: 2769.51 | backward-params-all-reduce: 22.64 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 5.34 | optimizer-clip-main-grad: 4.26 | optimizer-copy-main-to-model-params: 5.74 | optimizer: 38.59 | batch-generator: 2.21\n",
      "-------------------------------------------------------------------------------------------------\n",
      " validation loss at iteration 14000 | lm loss value: 1.452161E+00 | lm loss PPL: 4.272338E+00 | \n",
      "-------------------------------------------------------------------------------------------------\n",
      " iteration    14500/  125000 | consumed samples:       116000 | elapsed time per iteration (ms): 4004.9 | learning rate: 1.496E-04 | global batch size:     8 | lm loss: 6.745152E-01 | loss scale: 131072.0 | grad norm: 0.153 | number of skipped iterations:   2 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1170.62 | backward-compute: 2769.69 | backward-params-all-reduce: 22.43 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 5.35 | optimizer-clip-main-grad: 4.23 | optimizer-copy-main-to-model-params: 5.72 | optimizer: 38.48 | batch-generator: 2.41\n",
      " iteration    15000/  125000 | consumed samples:       120000 | elapsed time per iteration (ms): 3983.3 | learning rate: 1.495E-04 | global batch size:     8 | lm loss: 6.692257E-01 | loss scale: 32768.0 | grad norm: 0.143 | number of skipped iterations:   2 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1149.71 | backward-compute: 2769.24 | backward-params-all-reduce: 22.35 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 5.34 | optimizer-clip-main-grad: 4.23 | optimizer-copy-main-to-model-params: 5.60 | optimizer: 38.35 | batch-generator: 2.20\n",
      "-------------------------------------------------------------------------------------------------\n",
      " validation loss at iteration 15000 | lm loss value: 1.339231E+00 | lm loss PPL: 3.816109E+00 | \n",
      "-------------------------------------------------------------------------------------------------\n",
      "saving checkpoint at iteration   15000 to checkpoints/gpt_creditcard\n",
      "  successfully saved checkpoint at iteration   15000 to checkpoints/gpt_creditcard\n",
      "time (ms) | save-checkpoint: 11284.70\n",
      " iteration    15500/  125000 | consumed samples:       124000 | elapsed time per iteration (ms): 4027.4 | learning rate: 1.495E-04 | global batch size:     8 | lm loss: 6.700269E-01 | loss scale: 32768.0 | grad norm: 0.158 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1171.05 | backward-compute: 2768.89 | backward-params-all-reduce: 22.55 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 5.34 | optimizer-clip-main-grad: 4.25 | optimizer-copy-main-to-model-params: 5.74 | optimizer: 38.57 | batch-generator: 2.23\n",
      " iteration    16000/  125000 | consumed samples:       128000 | elapsed time per iteration (ms): 3983.3 | learning rate: 1.494E-04 | global batch size:     8 | lm loss: 6.701880E-01 | loss scale: 65536.0 | grad norm: 0.179 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1149.56 | backward-compute: 2769.06 | backward-params-all-reduce: 22.52 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 5.33 | optimizer-clip-main-grad: 4.23 | optimizer-copy-main-to-model-params: 5.72 | optimizer: 38.52 | batch-generator: 2.35\n",
      "-------------------------------------------------------------------------------------------------\n",
      " validation loss at iteration 16000 | lm loss value: 1.382074E+00 | lm loss PPL: 3.983153E+00 | \n",
      "-------------------------------------------------------------------------------------------------\n",
      " iteration    16500/  125000 | consumed samples:       132000 | elapsed time per iteration (ms): 4004.6 | learning rate: 1.494E-04 | global batch size:     8 | lm loss: 6.679370E-01 | loss scale: 65536.0 | grad norm: 0.145 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1170.75 | backward-compute: 2769.26 | backward-params-all-reduce: 22.44 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 5.33 | optimizer-clip-main-grad: 4.23 | optimizer-copy-main-to-model-params: 5.61 | optimizer: 38.42 | batch-generator: 2.36\n",
      " iteration    17000/  125000 | consumed samples:       136000 | elapsed time per iteration (ms): 3983.8 | learning rate: 1.493E-04 | global batch size:     8 | lm loss: 6.679150E-01 | loss scale: 65536.0 | grad norm: 0.141 | number of skipped iterations:   1 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1149.77 | backward-compute: 2769.26 | backward-params-all-reduce: 22.52 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 5.35 | optimizer-clip-main-grad: 4.27 | optimizer-copy-main-to-model-params: 5.73 | optimizer: 38.56 | batch-generator: 2.42\n",
      "-------------------------------------------------------------------------------------------------\n",
      " validation loss at iteration 17000 | lm loss value: 1.400654E+00 | lm loss PPL: 4.057851E+00 | \n",
      "-------------------------------------------------------------------------------------------------\n",
      " iteration    17500/  125000 | consumed samples:       140000 | elapsed time per iteration (ms): 4005.5 | learning rate: 1.493E-04 | global batch size:     8 | lm loss: 6.659003E-01 | loss scale: 65536.0 | grad norm: 0.176 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1171.36 | backward-compute: 2769.25 | backward-params-all-reduce: 22.54 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 5.35 | optimizer-clip-main-grad: 4.28 | optimizer-copy-main-to-model-params: 5.74 | optimizer: 38.63 | batch-generator: 2.31\n",
      " iteration    18000/  125000 | consumed samples:       144000 | elapsed time per iteration (ms): 3983.8 | learning rate: 1.493E-04 | global batch size:     8 | lm loss: 6.631312E-01 | loss scale: 131072.0 | grad norm: 0.152 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1149.69 | backward-compute: 2769.52 | backward-params-all-reduce: 22.40 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 5.34 | optimizer-clip-main-grad: 4.25 | optimizer-copy-main-to-model-params: 5.62 | optimizer: 38.46 | batch-generator: 2.20\n",
      "-------------------------------------------------------------------------------------------------\n",
      " validation loss at iteration 18000 | lm loss value: 1.288382E+00 | lm loss PPL: 3.626913E+00 | \n",
      "-------------------------------------------------------------------------------------------------\n",
      " iteration    18500/  125000 | consumed samples:       148000 | elapsed time per iteration (ms): 4005.5 | learning rate: 1.492E-04 | global batch size:     8 | lm loss: 6.615869E-01 | loss scale: 131072.0 | grad norm: 0.149 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1171.63 | backward-compute: 2769.60 | backward-params-all-reduce: 21.95 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 5.35 | optimizer-clip-main-grad: 4.25 | optimizer-copy-main-to-model-params: 5.74 | optimizer: 38.59 | batch-generator: 2.29\n",
      " iteration    19000/  125000 | consumed samples:       152000 | elapsed time per iteration (ms): 3983.8 | learning rate: 1.491E-04 | global batch size:     8 | lm loss: 6.622217E-01 | loss scale: 65536.0 | grad norm: 0.141 | number of skipped iterations:   3 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1150.06 | backward-compute: 2769.49 | backward-params-all-reduce: 22.11 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 5.35 | optimizer-clip-main-grad: 4.24 | optimizer-copy-main-to-model-params: 5.72 | optimizer: 38.45 | batch-generator: 2.20\n",
      "-------------------------------------------------------------------------------------------------\n",
      " validation loss at iteration 19000 | lm loss value: 1.367658E+00 | lm loss PPL: 3.926144E+00 | \n",
      "-------------------------------------------------------------------------------------------------\n",
      " iteration    19500/  125000 | consumed samples:       156000 | elapsed time per iteration (ms): 4003.8 | learning rate: 1.491E-04 | global batch size:     8 | lm loss: 6.624229E-01 | loss scale: 65536.0 | grad norm: 0.135 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1170.49 | backward-compute: 2769.21 | backward-params-all-reduce: 21.87 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 5.35 | optimizer-clip-main-grad: 4.26 | optimizer-copy-main-to-model-params: 5.63 | optimizer: 38.50 | batch-generator: 2.26\n",
      " iteration    20000/  125000 | consumed samples:       160000 | elapsed time per iteration (ms): 3984.5 | learning rate: 1.490E-04 | global batch size:     8 | lm loss: 6.594308E-01 | loss scale: 131072.0 | grad norm: 0.158 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1150.73 | backward-compute: 2769.40 | backward-params-all-reduce: 22.00 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 5.35 | optimizer-clip-main-grad: 4.27 | optimizer-copy-main-to-model-params: 5.74 | optimizer: 38.61 | batch-generator: 2.28\n",
      "-------------------------------------------------------------------------------------------------\n",
      " validation loss at iteration 20000 | lm loss value: 1.321931E+00 | lm loss PPL: 3.750656E+00 | \n",
      "-------------------------------------------------------------------------------------------------\n",
      "saving checkpoint at iteration   20000 to checkpoints/gpt_creditcard\n",
      "  successfully saved checkpoint at iteration   20000 to checkpoints/gpt_creditcard\n",
      "time (ms) | save-checkpoint: 11441.76\n",
      " iteration    20500/  125000 | consumed samples:       164000 | elapsed time per iteration (ms): 4028.0 | learning rate: 1.490E-04 | global batch size:     8 | lm loss: 6.572558E-01 | loss scale: 131072.0 | grad norm: 0.143 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1170.98 | backward-compute: 2769.59 | backward-params-all-reduce: 22.26 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 5.35 | optimizer-clip-main-grad: 4.26 | optimizer-copy-main-to-model-params: 5.73 | optimizer: 38.60 | batch-generator: 2.26\n",
      " iteration    21000/  125000 | consumed samples:       168000 | elapsed time per iteration (ms): 3983.8 | learning rate: 1.489E-04 | global batch size:     8 | lm loss: 6.592191E-01 | loss scale: 262144.0 | grad norm: 0.148 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1149.66 | backward-compute: 2769.72 | backward-params-all-reduce: 22.29 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 5.34 | optimizer-clip-main-grad: 4.25 | optimizer-copy-main-to-model-params: 5.62 | optimizer: 38.46 | batch-generator: 2.26\n",
      "-------------------------------------------------------------------------------------------------\n",
      " validation loss at iteration 21000 | lm loss value: 1.277580E+00 | lm loss PPL: 3.587945E+00 | \n",
      "-------------------------------------------------------------------------------------------------\n",
      " iteration    21500/  125000 | consumed samples:       172000 | elapsed time per iteration (ms): 4004.9 | learning rate: 1.489E-04 | global batch size:     8 | lm loss: 6.556589E-01 | loss scale: 65536.0 | grad norm: 0.134 | number of skipped iterations:   3 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1170.92 | backward-compute: 2769.52 | backward-params-all-reduce: 22.28 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 5.35 | optimizer-clip-main-grad: 4.22 | optimizer-copy-main-to-model-params: 5.70 | optimizer: 38.41 | batch-generator: 2.29\n",
      " iteration    22000/  125000 | consumed samples:       176000 | elapsed time per iteration (ms): 3983.6 | learning rate: 1.488E-04 | global batch size:     8 | lm loss: 6.583307E-01 | loss scale: 65536.0 | grad norm: 0.197 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1149.82 | backward-compute: 2769.17 | backward-params-all-reduce: 22.38 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 5.34 | optimizer-clip-main-grad: 4.25 | optimizer-copy-main-to-model-params: 5.73 | optimizer: 38.57 | batch-generator: 2.20\n",
      "-------------------------------------------------------------------------------------------------\n",
      " validation loss at iteration 22000 | lm loss value: 1.340467E+00 | lm loss PPL: 3.820826E+00 | \n",
      "-------------------------------------------------------------------------------------------------\n",
      " iteration    22500/  125000 | consumed samples:       180000 | elapsed time per iteration (ms): 4004.5 | learning rate: 1.487E-04 | global batch size:     8 | lm loss: 6.566702E-01 | loss scale: 131072.0 | grad norm: 0.137 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1170.82 | backward-compute: 2769.13 | backward-params-all-reduce: 22.32 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 5.35 | optimizer-clip-main-grad: 4.26 | optimizer-copy-main-to-model-params: 5.62 | optimizer: 38.49 | batch-generator: 2.27\n",
      " iteration    23000/  125000 | consumed samples:       184000 | elapsed time per iteration (ms): 3983.8 | learning rate: 1.487E-04 | global batch size:     8 | lm loss: 6.561987E-01 | loss scale: 131072.0 | grad norm: 0.133 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1149.74 | backward-compute: 2769.35 | backward-params-all-reduce: 22.38 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.17 | optimizer-unscale-and-check-inf: 5.35 | optimizer-clip-main-grad: 4.26 | optimizer-copy-main-to-model-params: 5.74 | optimizer: 38.59 | batch-generator: 2.22\n",
      "-------------------------------------------------------------------------------------------------\n",
      " validation loss at iteration 23000 | lm loss value: 1.299970E+00 | lm loss PPL: 3.669188E+00 | \n",
      "-------------------------------------------------------------------------------------------------\n",
      " iteration    23500/  125000 | consumed samples:       188000 | elapsed time per iteration (ms): 4004.2 | learning rate: 1.486E-04 | global batch size:     8 | lm loss: 6.549226E-01 | loss scale: 32768.0 | grad norm: 0.153 | number of skipped iterations:   3 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1170.88 | backward-compute: 2768.98 | backward-params-all-reduce: 22.16 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 5.35 | optimizer-clip-main-grad: 4.23 | optimizer-copy-main-to-model-params: 5.71 | optimizer: 38.42 | batch-generator: 2.21\n",
      " iteration    24000/  125000 | consumed samples:       192000 | elapsed time per iteration (ms): 3982.9 | learning rate: 1.485E-04 | global batch size:     8 | lm loss: 6.552836E-01 | loss scale: 32768.0 | grad norm: 0.139 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1149.85 | backward-compute: 2768.84 | backward-params-all-reduce: 22.06 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 5.34 | optimizer-clip-main-grad: 4.29 | optimizer-copy-main-to-model-params: 5.62 | optimizer: 38.50 | batch-generator: 2.25\n",
      "-------------------------------------------------------------------------------------------------\n",
      " validation loss at iteration 24000 | lm loss value: 1.289448E+00 | lm loss PPL: 3.630784E+00 | \n",
      "-------------------------------------------------------------------------------------------------\n",
      " iteration    24500/  125000 | consumed samples:       196000 | elapsed time per iteration (ms): 4003.9 | learning rate: 1.485E-04 | global batch size:     8 | lm loss: 6.550297E-01 | loss scale: 65536.0 | grad norm: 0.153 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1170.44 | backward-compute: 2769.16 | backward-params-all-reduce: 21.97 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 5.34 | optimizer-clip-main-grad: 4.25 | optimizer-copy-main-to-model-params: 5.74 | optimizer: 38.58 | batch-generator: 2.26\n",
      " iteration    25000/  125000 | consumed samples:       200000 | elapsed time per iteration (ms): 3983.6 | learning rate: 1.484E-04 | global batch size:     8 | lm loss: 6.522876E-01 | loss scale: 65536.0 | grad norm: 0.132 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1150.09 | backward-compute: 2769.19 | backward-params-all-reduce: 22.10 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 5.34 | optimizer-clip-main-grad: 4.24 | optimizer-copy-main-to-model-params: 5.73 | optimizer: 38.57 | batch-generator: 2.20\n",
      "-------------------------------------------------------------------------------------------------\n",
      " validation loss at iteration 25000 | lm loss value: 1.290043E+00 | lm loss PPL: 3.632941E+00 | \n",
      "-------------------------------------------------------------------------------------------------\n",
      "saving checkpoint at iteration   25000 to checkpoints/gpt_creditcard\n",
      "  successfully saved checkpoint at iteration   25000 to checkpoints/gpt_creditcard\n",
      "time (ms) | save-checkpoint: 11823.90\n",
      " iteration    25500/  125000 | consumed samples:       204000 | elapsed time per iteration (ms): 4028.0 | learning rate: 1.483E-04 | global batch size:     8 | lm loss: 6.518653E-01 | loss scale: 131072.0 | grad norm: 0.158 | number of skipped iterations:   1 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1170.79 | backward-compute: 2769.37 | backward-params-all-reduce: 22.05 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.17 | optimizer-unscale-and-check-inf: 5.35 | optimizer-clip-main-grad: 4.27 | optimizer-copy-main-to-model-params: 5.61 | optimizer: 38.46 | batch-generator: 2.28\n",
      " iteration    26000/  125000 | consumed samples:       208000 | elapsed time per iteration (ms): 3984.0 | learning rate: 1.482E-04 | global batch size:     8 | lm loss: 6.513991E-01 | loss scale: 131072.0 | grad norm: 0.129 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1150.20 | backward-compute: 2769.43 | backward-params-all-reduce: 22.10 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 5.34 | optimizer-clip-main-grad: 4.25 | optimizer-copy-main-to-model-params: 5.73 | optimizer: 38.58 | batch-generator: 2.30\n",
      "-------------------------------------------------------------------------------------------------\n",
      " validation loss at iteration 26000 | lm loss value: 1.329905E+00 | lm loss PPL: 3.780683E+00 | \n",
      "-------------------------------------------------------------------------------------------------\n",
      " iteration    26500/  125000 | consumed samples:       212000 | elapsed time per iteration (ms): 4006.0 | learning rate: 1.481E-04 | global batch size:     8 | lm loss: 6.517541E-01 | loss scale: 262144.0 | grad norm: 0.135 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1172.11 | backward-compute: 2769.51 | backward-params-all-reduce: 22.10 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 5.17 | optimizer-unscale-and-check-inf: 5.34 | optimizer-clip-main-grad: 4.24 | optimizer-copy-main-to-model-params: 5.73 | optimizer: 38.56 | batch-generator: 2.23\n",
      " iteration    27000/  125000 | consumed samples:       216000 | elapsed time per iteration (ms): 3984.2 | learning rate: 1.481E-04 | global batch size:     8 | lm loss: 6.487388E-01 | loss scale: 262144.0 | grad norm: 0.124 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1150.24 | backward-compute: 2769.80 | backward-params-all-reduce: 22.01 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 5.34 | optimizer-clip-main-grad: 4.24 | optimizer-copy-main-to-model-params: 5.62 | optimizer: 38.44 | batch-generator: 2.27\n",
      "-------------------------------------------------------------------------------------------------\n",
      " validation loss at iteration 27000 | lm loss value: 1.308446E+00 | lm loss PPL: 3.700420E+00 | \n",
      "-------------------------------------------------------------------------------------------------\n",
      " iteration    27500/  125000 | consumed samples:       220000 | elapsed time per iteration (ms): 4004.8 | learning rate: 1.480E-04 | global batch size:     8 | lm loss: 6.483406E-01 | loss scale: 131072.0 | grad norm: 0.116 | number of skipped iterations:   2 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1171.02 | backward-compute: 2769.50 | backward-params-all-reduce: 22.12 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 5.34 | optimizer-clip-main-grad: 4.23 | optimizer-copy-main-to-model-params: 5.71 | optimizer: 38.46 | batch-generator: 2.34\n",
      " iteration    28000/  125000 | consumed samples:       224000 | elapsed time per iteration (ms): 3983.6 | learning rate: 1.479E-04 | global batch size:     8 | lm loss: 6.512134E-01 | loss scale: 65536.0 | grad norm: 0.121 | number of skipped iterations:   1 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1149.79 | backward-compute: 2769.41 | backward-params-all-reduce: 22.09 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 5.35 | optimizer-clip-main-grad: 4.26 | optimizer-copy-main-to-model-params: 5.74 | optimizer: 38.58 | batch-generator: 2.33\n",
      "-------------------------------------------------------------------------------------------------\n",
      " validation loss at iteration 28000 | lm loss value: 1.337571E+00 | lm loss PPL: 3.809778E+00 | \n",
      "-------------------------------------------------------------------------------------------------\n",
      " iteration    28500/  125000 | consumed samples:       228000 | elapsed time per iteration (ms): 4004.3 | learning rate: 1.478E-04 | global batch size:     8 | lm loss: 6.509706E-01 | loss scale: 65536.0 | grad norm: 0.131 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1170.84 | backward-compute: 2769.25 | backward-params-all-reduce: 22.05 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 5.34 | optimizer-clip-main-grad: 4.25 | optimizer-copy-main-to-model-params: 5.62 | optimizer: 38.46 | batch-generator: 2.30\n",
      " iteration    29000/  125000 | consumed samples:       232000 | elapsed time per iteration (ms): 3983.5 | learning rate: 1.477E-04 | global batch size:     8 | lm loss: 6.505139E-01 | loss scale: 131072.0 | grad norm: 0.123 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1149.80 | backward-compute: 2769.22 | backward-params-all-reduce: 22.17 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 5.35 | optimizer-clip-main-grad: 4.25 | optimizer-copy-main-to-model-params: 5.74 | optimizer: 38.57 | batch-generator: 2.28\n",
      "-------------------------------------------------------------------------------------------------\n",
      " validation loss at iteration 29000 | lm loss value: 1.345449E+00 | lm loss PPL: 3.839910E+00 | \n",
      "-------------------------------------------------------------------------------------------------\n",
      " iteration    29500/  125000 | consumed samples:       236000 | elapsed time per iteration (ms): 4004.5 | learning rate: 1.476E-04 | global batch size:     8 | lm loss: 6.487924E-01 | loss scale: 131072.0 | grad norm: 0.117 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1170.69 | backward-compute: 2769.52 | backward-params-all-reduce: 22.07 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 5.34 | optimizer-clip-main-grad: 4.23 | optimizer-copy-main-to-model-params: 5.73 | optimizer: 38.53 | batch-generator: 2.20\n",
      " iteration    30000/  125000 | consumed samples:       240000 | elapsed time per iteration (ms): 3983.3 | learning rate: 1.476E-04 | global batch size:     8 | lm loss: 6.488895E-01 | loss scale: 262144.0 | grad norm: 0.130 | number of skipped iterations:   1 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1149.64 | backward-compute: 2769.63 | backward-params-all-reduce: 21.91 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 5.34 | optimizer-clip-main-grad: 4.23 | optimizer-copy-main-to-model-params: 5.61 | optimizer: 38.39 | batch-generator: 2.20\n",
      "-------------------------------------------------------------------------------------------------\n",
      " validation loss at iteration 30000 | lm loss value: 1.282952E+00 | lm loss PPL: 3.607273E+00 | \n",
      "-------------------------------------------------------------------------------------------------\n",
      "saving checkpoint at iteration   30000 to checkpoints/gpt_creditcard\n",
      "  successfully saved checkpoint at iteration   30000 to checkpoints/gpt_creditcard\n",
      "time (ms) | save-checkpoint: 11599.32\n",
      " iteration    30500/  125000 | consumed samples:       244000 | elapsed time per iteration (ms): 4027.3 | learning rate: 1.475E-04 | global batch size:     8 | lm loss: 6.460604E-01 | loss scale: 131072.0 | grad norm: 0.114 | number of skipped iterations:   1 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1170.47 | backward-compute: 2769.60 | backward-params-all-reduce: 21.73 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 5.34 | optimizer-clip-main-grad: 4.24 | optimizer-copy-main-to-model-params: 5.73 | optimizer: 38.54 | batch-generator: 2.26\n",
      " iteration    31000/  125000 | consumed samples:       248000 | elapsed time per iteration (ms): 3983.0 | learning rate: 1.474E-04 | global batch size:     8 | lm loss: 6.478620E-01 | loss scale: 131072.0 | grad norm: 0.110 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1149.42 | backward-compute: 2769.43 | backward-params-all-reduce: 21.93 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 5.17 | optimizer-unscale-and-check-inf: 5.35 | optimizer-clip-main-grad: 4.25 | optimizer-copy-main-to-model-params: 5.73 | optimizer: 38.58 | batch-generator: 2.22\n",
      "-------------------------------------------------------------------------------------------------\n",
      " validation loss at iteration 31000 | lm loss value: 1.321595E+00 | lm loss PPL: 3.749398E+00 | \n",
      "-------------------------------------------------------------------------------------------------\n",
      " iteration    31500/  125000 | consumed samples:       252000 | elapsed time per iteration (ms): 4003.6 | learning rate: 1.473E-04 | global batch size:     8 | lm loss: 6.463117E-01 | loss scale: 131072.0 | grad norm: 0.113 | number of skipped iterations:   2 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1170.19 | backward-compute: 2769.47 | backward-params-all-reduce: 21.87 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 5.34 | optimizer-clip-main-grad: 4.24 | optimizer-copy-main-to-model-params: 5.60 | optimizer: 38.36 | batch-generator: 2.26\n",
      " iteration    32000/  125000 | consumed samples:       256000 | elapsed time per iteration (ms): 3983.0 | learning rate: 1.472E-04 | global batch size:     8 | lm loss: 6.447723E-01 | loss scale: 65536.0 | grad norm: 0.097 | number of skipped iterations:   1 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1149.48 | backward-compute: 2769.38 | backward-params-all-reduce: 21.92 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 5.34 | optimizer-clip-main-grad: 4.24 | optimizer-copy-main-to-model-params: 5.72 | optimizer: 38.51 | batch-generator: 2.23\n",
      "-------------------------------------------------------------------------------------------------\n",
      " validation loss at iteration 32000 | lm loss value: 1.321556E+00 | lm loss PPL: 3.749250E+00 | \n",
      "-------------------------------------------------------------------------------------------------\n",
      " iteration    32500/  125000 | consumed samples:       260000 | elapsed time per iteration (ms): 4004.0 | learning rate: 1.471E-04 | global batch size:     8 | lm loss: 6.460622E-01 | loss scale: 65536.0 | grad norm: 0.106 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1170.69 | backward-compute: 2769.19 | backward-params-all-reduce: 21.86 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 5.34 | optimizer-clip-main-grad: 4.24 | optimizer-copy-main-to-model-params: 5.73 | optimizer: 38.55 | batch-generator: 2.31\n",
      " iteration    33000/  125000 | consumed samples:       264000 | elapsed time per iteration (ms): 3982.4 | learning rate: 1.470E-04 | global batch size:     8 | lm loss: 6.447667E-01 | loss scale: 131072.0 | grad norm: 0.119 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1149.16 | backward-compute: 2769.24 | backward-params-all-reduce: 21.84 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.17 | optimizer-unscale-and-check-inf: 5.34 | optimizer-clip-main-grad: 4.24 | optimizer-copy-main-to-model-params: 5.62 | optimizer: 38.45 | batch-generator: 2.24\n",
      "-------------------------------------------------------------------------------------------------\n",
      " validation loss at iteration 33000 | lm loss value: 1.266375E+00 | lm loss PPL: 3.547967E+00 | \n",
      "-------------------------------------------------------------------------------------------------\n",
      " iteration    33500/  125000 | consumed samples:       268000 | elapsed time per iteration (ms): 4004.1 | learning rate: 1.469E-04 | global batch size:     8 | lm loss: 6.450590E-01 | loss scale: 131072.0 | grad norm: 0.112 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1170.56 | backward-compute: 2769.42 | backward-params-all-reduce: 21.85 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 5.34 | optimizer-clip-main-grad: 4.25 | optimizer-copy-main-to-model-params: 5.73 | optimizer: 38.56 | batch-generator: 2.24\n",
      " iteration    34000/  125000 | consumed samples:       272000 | elapsed time per iteration (ms): 3982.8 | learning rate: 1.468E-04 | global batch size:     8 | lm loss: 6.434513E-01 | loss scale: 65536.0 | grad norm: 0.113 | number of skipped iterations:   2 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1149.44 | backward-compute: 2769.34 | backward-params-all-reduce: 21.86 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 5.35 | optimizer-clip-main-grad: 4.24 | optimizer-copy-main-to-model-params: 5.72 | optimizer: 38.49 | batch-generator: 2.21\n",
      "-------------------------------------------------------------------------------------------------\n",
      " validation loss at iteration 34000 | lm loss value: 1.306664E+00 | lm loss PPL: 3.693831E+00 | \n",
      "-------------------------------------------------------------------------------------------------\n",
      " iteration    34500/  125000 | consumed samples:       276000 | elapsed time per iteration (ms): 4003.7 | learning rate: 1.467E-04 | global batch size:     8 | lm loss: 6.451526E-01 | loss scale: 65536.0 | grad norm: 0.111 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1170.38 | backward-compute: 2769.11 | backward-params-all-reduce: 21.88 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 5.34 | optimizer-clip-main-grad: 4.24 | optimizer-copy-main-to-model-params: 5.75 | optimizer: 38.57 | batch-generator: 2.25\n",
      " iteration    35000/  125000 | consumed samples:       280000 | elapsed time per iteration (ms): 3982.9 | learning rate: 1.466E-04 | global batch size:     8 | lm loss: 6.451840E-01 | loss scale: 131072.0 | grad norm: 0.120 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1149.64 | backward-compute: 2769.29 | backward-params-all-reduce: 21.79 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.17 | optimizer-unscale-and-check-inf: 5.35 | optimizer-clip-main-grad: 4.25 | optimizer-copy-main-to-model-params: 5.61 | optimizer: 38.47 | batch-generator: 2.26\n",
      "-------------------------------------------------------------------------------------------------\n",
      " validation loss at iteration 35000 | lm loss value: 1.306388E+00 | lm loss PPL: 3.692810E+00 | \n",
      "-------------------------------------------------------------------------------------------------\n",
      "saving checkpoint at iteration   35000 to checkpoints/gpt_creditcard\n",
      "  successfully saved checkpoint at iteration   35000 to checkpoints/gpt_creditcard\n",
      "time (ms) | save-checkpoint: 11760.04\n",
      " iteration    35500/  125000 | consumed samples:       284000 | elapsed time per iteration (ms): 4027.8 | learning rate: 1.465E-04 | global batch size:     8 | lm loss: 6.428484E-01 | loss scale: 131072.0 | grad norm: 0.126 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1170.74 | backward-compute: 2769.31 | backward-params-all-reduce: 22.01 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 5.34 | optimizer-clip-main-grad: 4.23 | optimizer-copy-main-to-model-params: 5.74 | optimizer: 38.54 | batch-generator: 2.24\n",
      " iteration    36000/  125000 | consumed samples:       288000 | elapsed time per iteration (ms): 3982.5 | learning rate: 1.463E-04 | global batch size:     8 | lm loss: 6.448276E-01 | loss scale: 32768.0 | grad norm: 0.095 | number of skipped iterations:   3 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1149.44 | backward-compute: 2768.93 | backward-params-all-reduce: 22.01 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 5.34 | optimizer-clip-main-grad: 4.22 | optimizer-copy-main-to-model-params: 5.72 | optimizer: 38.40 | batch-generator: 2.20\n",
      "-------------------------------------------------------------------------------------------------\n",
      " validation loss at iteration 36000 | lm loss value: 1.320572E+00 | lm loss PPL: 3.745563E+00 | \n",
      "-------------------------------------------------------------------------------------------------\n",
      " iteration    36500/  125000 | consumed samples:       292000 | elapsed time per iteration (ms): 4003.3 | learning rate: 1.462E-04 | global batch size:     8 | lm loss: 6.466904E-01 | loss scale: 32768.0 | grad norm: 0.129 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1170.43 | backward-compute: 2768.79 | backward-params-all-reduce: 21.88 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 5.34 | optimizer-clip-main-grad: 4.25 | optimizer-copy-main-to-model-params: 5.61 | optimizer: 38.44 | batch-generator: 2.31\n",
      " iteration    37000/  125000 | consumed samples:       296000 | elapsed time per iteration (ms): 3982.8 | learning rate: 1.461E-04 | global batch size:     8 | lm loss: 6.426727E-01 | loss scale: 65536.0 | grad norm: 0.122 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1149.56 | backward-compute: 2768.97 | backward-params-all-reduce: 21.88 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.17 | optimizer-unscale-and-check-inf: 5.35 | optimizer-clip-main-grad: 4.25 | optimizer-copy-main-to-model-params: 5.74 | optimizer: 38.60 | batch-generator: 2.27\n",
      "-------------------------------------------------------------------------------------------------\n",
      " validation loss at iteration 37000 | lm loss value: 1.333040E+00 | lm loss PPL: 3.792556E+00 | \n",
      "-------------------------------------------------------------------------------------------------\n",
      " iteration    37500/  125000 | consumed samples:       300000 | elapsed time per iteration (ms): 4003.8 | learning rate: 1.460E-04 | global batch size:     8 | lm loss: 6.433123E-01 | loss scale: 65536.0 | grad norm: 0.110 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1170.43 | backward-compute: 2769.07 | backward-params-all-reduce: 21.93 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 5.35 | optimizer-clip-main-grad: 4.25 | optimizer-copy-main-to-model-params: 5.76 | optimizer: 38.61 | batch-generator: 2.28\n",
      " iteration    38000/  125000 | consumed samples:       304000 | elapsed time per iteration (ms): 3982.6 | learning rate: 1.459E-04 | global batch size:     8 | lm loss: 6.426202E-01 | loss scale: 131072.0 | grad norm: 0.107 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1149.24 | backward-compute: 2769.34 | backward-params-all-reduce: 21.83 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.17 | optimizer-unscale-and-check-inf: 5.35 | optimizer-clip-main-grad: 4.25 | optimizer-copy-main-to-model-params: 5.62 | optimizer: 38.48 | batch-generator: 2.23\n",
      "-------------------------------------------------------------------------------------------------\n",
      " validation loss at iteration 38000 | lm loss value: 1.301062E+00 | lm loss PPL: 3.673195E+00 | \n",
      "-------------------------------------------------------------------------------------------------\n",
      " iteration    38500/  125000 | consumed samples:       308000 | elapsed time per iteration (ms): 4004.2 | learning rate: 1.458E-04 | global batch size:     8 | lm loss: 6.416920E-01 | loss scale: 131072.0 | grad norm: 0.098 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1170.48 | backward-compute: 2769.34 | backward-params-all-reduce: 22.01 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.17 | optimizer-unscale-and-check-inf: 5.35 | optimizer-clip-main-grad: 4.27 | optimizer-copy-main-to-model-params: 5.75 | optimizer: 38.63 | batch-generator: 2.32\n",
      " iteration    39000/  125000 | consumed samples:       312000 | elapsed time per iteration (ms): 3982.8 | learning rate: 1.457E-04 | global batch size:     8 | lm loss: 6.441013E-01 | loss scale: 32768.0 | grad norm: 0.106 | number of skipped iterations:   4 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1149.52 | backward-compute: 2769.18 | backward-params-all-reduce: 21.96 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 5.35 | optimizer-clip-main-grad: 4.23 | optimizer-copy-main-to-model-params: 5.72 | optimizer: 38.40 | batch-generator: 2.27\n",
      "-------------------------------------------------------------------------------------------------\n",
      " validation loss at iteration 39000 | lm loss value: 1.301283E+00 | lm loss PPL: 3.674008E+00 | \n",
      "-------------------------------------------------------------------------------------------------\n",
      " iteration    39500/  125000 | consumed samples:       316000 | elapsed time per iteration (ms): 4003.3 | learning rate: 1.455E-04 | global batch size:     8 | lm loss: 6.421680E-01 | loss scale: 32768.0 | grad norm: 0.109 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1170.29 | backward-compute: 2768.86 | backward-params-all-reduce: 21.82 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 5.36 | optimizer-clip-main-grad: 4.28 | optimizer-copy-main-to-model-params: 5.62 | optimizer: 38.54 | batch-generator: 2.24\n",
      " iteration    40000/  125000 | consumed samples:       320000 | elapsed time per iteration (ms): 3982.4 | learning rate: 1.454E-04 | global batch size:     8 | lm loss: 6.421636E-01 | loss scale: 65536.0 | grad norm: 0.101 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1149.37 | backward-compute: 2768.82 | backward-params-all-reduce: 21.88 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 5.36 | optimizer-clip-main-grad: 4.27 | optimizer-copy-main-to-model-params: 5.75 | optimizer: 38.64 | batch-generator: 2.27\n",
      "-------------------------------------------------------------------------------------------------\n",
      " validation loss at iteration 40000 | lm loss value: 1.343905E+00 | lm loss PPL: 3.833988E+00 | \n",
      "-------------------------------------------------------------------------------------------------\n",
      "saving checkpoint at iteration   40000 to checkpoints/gpt_creditcard\n",
      "  successfully saved checkpoint at iteration   40000 to checkpoints/gpt_creditcard\n",
      "time (ms) | save-checkpoint: 11670.92\n",
      " iteration    40500/  125000 | consumed samples:       324000 | elapsed time per iteration (ms): 4027.4 | learning rate: 1.453E-04 | global batch size:     8 | lm loss: 6.425362E-01 | loss scale: 65536.0 | grad norm: 0.105 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1170.94 | backward-compute: 2769.04 | backward-params-all-reduce: 21.72 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 5.36 | optimizer-clip-main-grad: 4.27 | optimizer-copy-main-to-model-params: 5.76 | optimizer: 38.66 | batch-generator: 2.31\n",
      " iteration    41000/  125000 | consumed samples:       328000 | elapsed time per iteration (ms): 3982.5 | learning rate: 1.452E-04 | global batch size:     8 | lm loss: 6.399341E-01 | loss scale: 131072.0 | grad norm: 0.104 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1149.39 | backward-compute: 2769.09 | backward-params-all-reduce: 21.72 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 5.36 | optimizer-clip-main-grad: 4.28 | optimizer-copy-main-to-model-params: 5.63 | optimizer: 38.53 | batch-generator: 2.26\n",
      "-------------------------------------------------------------------------------------------------\n",
      " validation loss at iteration 41000 | lm loss value: 1.292508E+00 | lm loss PPL: 3.641910E+00 | \n",
      "-------------------------------------------------------------------------------------------------\n",
      " iteration    41500/  125000 | consumed samples:       332000 | elapsed time per iteration (ms): 4003.6 | learning rate: 1.450E-04 | global batch size:     8 | lm loss: 6.412930E-01 | loss scale: 131072.0 | grad norm: 0.100 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1170.15 | backward-compute: 2769.33 | backward-params-all-reduce: 21.77 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 5.35 | optimizer-clip-main-grad: 4.26 | optimizer-copy-main-to-model-params: 5.74 | optimizer: 38.61 | batch-generator: 2.32\n",
      " iteration    42000/  125000 | consumed samples:       336000 | elapsed time per iteration (ms): 3983.1 | learning rate: 1.449E-04 | global batch size:     8 | lm loss: 6.403029E-01 | loss scale: 262144.0 | grad norm: 0.116 | number of skipped iterations:   1 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1149.47 | backward-compute: 2769.48 | backward-params-all-reduce: 21.83 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 5.36 | optimizer-clip-main-grad: 4.27 | optimizer-copy-main-to-model-params: 5.74 | optimizer: 38.60 | batch-generator: 2.21\n",
      "-------------------------------------------------------------------------------------------------\n",
      " validation loss at iteration 42000 | lm loss value: 1.313965E+00 | lm loss PPL: 3.720898E+00 | \n",
      "-------------------------------------------------------------------------------------------------\n",
      " iteration    42500/  125000 | consumed samples:       340000 | elapsed time per iteration (ms): 4005.0 | learning rate: 1.448E-04 | global batch size:     8 | lm loss: 6.397451E-01 | loss scale: 131072.0 | grad norm: 0.110 | number of skipped iterations:   1 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1170.66 | backward-compute: 2769.87 | backward-params-all-reduce: 22.21 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.17 | optimizer-unscale-and-check-inf: 5.36 | optimizer-clip-main-grad: 4.27 | optimizer-copy-main-to-model-params: 5.61 | optimizer: 38.48 | batch-generator: 2.29\n",
      " iteration    43000/  125000 | consumed samples:       344000 | elapsed time per iteration (ms): 3983.4 | learning rate: 1.446E-04 | global batch size:     8 | lm loss: 6.398533E-01 | loss scale: 131072.0 | grad norm: 0.095 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1149.21 | backward-compute: 2769.73 | backward-params-all-reduce: 22.11 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 5.36 | optimizer-clip-main-grad: 4.26 | optimizer-copy-main-to-model-params: 5.76 | optimizer: 38.63 | batch-generator: 2.24\n",
      "-------------------------------------------------------------------------------------------------\n",
      " validation loss at iteration 43000 | lm loss value: 1.279472E+00 | lm loss PPL: 3.594740E+00 | \n",
      "-------------------------------------------------------------------------------------------------\n",
      " iteration    43500/  125000 | consumed samples:       348000 | elapsed time per iteration (ms): 4005.7 | learning rate: 1.445E-04 | global batch size:     8 | lm loss: 6.392839E-01 | loss scale: 262144.0 | grad norm: 0.107 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1171.41 | backward-compute: 2769.74 | backward-params-all-reduce: 22.23 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 5.36 | optimizer-clip-main-grad: 4.26 | optimizer-copy-main-to-model-params: 5.75 | optimizer: 38.62 | batch-generator: 2.29\n",
      " iteration    44000/  125000 | consumed samples:       352000 | elapsed time per iteration (ms): 3983.7 | learning rate: 1.444E-04 | global batch size:     8 | lm loss: 6.415688E-01 | loss scale: 65536.0 | grad norm: 0.106 | number of skipped iterations:   3 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1149.94 | backward-compute: 2769.63 | backward-params-all-reduce: 22.20 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 5.33 | optimizer-clip-main-grad: 4.21 | optimizer-copy-main-to-model-params: 5.58 | optimizer: 38.26 | batch-generator: 2.24\n",
      "-------------------------------------------------------------------------------------------------\n",
      " validation loss at iteration 44000 | lm loss value: 1.267458E+00 | lm loss PPL: 3.551814E+00 | \n",
      "-------------------------------------------------------------------------------------------------\n",
      " iteration    44500/  125000 | consumed samples:       356000 | elapsed time per iteration (ms): 4004.5 | learning rate: 1.442E-04 | global batch size:     8 | lm loss: 6.414609E-01 | loss scale: 65536.0 | grad norm: 0.101 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1170.63 | backward-compute: 2769.29 | backward-params-all-reduce: 22.28 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 5.34 | optimizer-clip-main-grad: 4.25 | optimizer-copy-main-to-model-params: 5.74 | optimizer: 38.58 | batch-generator: 2.27\n",
      " iteration    45000/  125000 | consumed samples:       360000 | elapsed time per iteration (ms): 3983.5 | learning rate: 1.441E-04 | global batch size:     8 | lm loss: 6.375506E-01 | loss scale: 131072.0 | grad norm: 0.102 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1149.47 | backward-compute: 2769.50 | backward-params-all-reduce: 22.21 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 5.35 | optimizer-clip-main-grad: 4.26 | optimizer-copy-main-to-model-params: 5.75 | optimizer: 38.60 | batch-generator: 2.29\n",
      "-------------------------------------------------------------------------------------------------\n",
      " validation loss at iteration 45000 | lm loss value: 1.345687E+00 | lm loss PPL: 3.840826E+00 | \n",
      "-------------------------------------------------------------------------------------------------\n",
      "saving checkpoint at iteration   45000 to checkpoints/gpt_creditcard\n",
      "  successfully saved checkpoint at iteration   45000 to checkpoints/gpt_creditcard\n",
      "time (ms) | save-checkpoint: 11650.04\n",
      " iteration    45500/  125000 | consumed samples:       364000 | elapsed time per iteration (ms): 4028.1 | learning rate: 1.440E-04 | global batch size:     8 | lm loss: 6.363462E-01 | loss scale: 131072.0 | grad norm: 0.112 | number of skipped iterations:   1 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1170.89 | backward-compute: 2769.55 | backward-params-all-reduce: 22.22 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 5.34 | optimizer-clip-main-grad: 4.24 | optimizer-copy-main-to-model-params: 5.61 | optimizer: 38.40 | batch-generator: 2.31\n",
      " iteration    46000/  125000 | consumed samples:       368000 | elapsed time per iteration (ms): 3984.1 | learning rate: 1.438E-04 | global batch size:     8 | lm loss: 6.392073E-01 | loss scale: 131072.0 | grad norm: 0.098 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1149.93 | backward-compute: 2769.62 | backward-params-all-reduce: 22.24 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 5.35 | optimizer-clip-main-grad: 4.25 | optimizer-copy-main-to-model-params: 5.74 | optimizer: 38.59 | batch-generator: 2.27\n",
      "-------------------------------------------------------------------------------------------------\n",
      " validation loss at iteration 46000 | lm loss value: 1.272771E+00 | lm loss PPL: 3.570735E+00 | \n",
      "-------------------------------------------------------------------------------------------------\n",
      " iteration    46500/  125000 | consumed samples:       372000 | elapsed time per iteration (ms): 4004.2 | learning rate: 1.437E-04 | global batch size:     8 | lm loss: 6.384921E-01 | loss scale: 65536.0 | grad norm: 0.101 | number of skipped iterations:   1 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1170.34 | backward-compute: 2769.41 | backward-params-all-reduce: 22.11 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 5.35 | optimizer-clip-main-grad: 4.26 | optimizer-copy-main-to-model-params: 5.74 | optimizer: 38.57 | batch-generator: 2.43\n",
      " iteration    47000/  125000 | consumed samples:       376000 | elapsed time per iteration (ms): 3983.7 | learning rate: 1.435E-04 | global batch size:     8 | lm loss: 6.398829E-01 | loss scale: 65536.0 | grad norm: 0.100 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1150.02 | backward-compute: 2769.32 | backward-params-all-reduce: 22.16 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.17 | optimizer-unscale-and-check-inf: 5.34 | optimizer-clip-main-grad: 4.25 | optimizer-copy-main-to-model-params: 5.62 | optimizer: 38.46 | batch-generator: 2.29\n",
      "-------------------------------------------------------------------------------------------------\n",
      " validation loss at iteration 47000 | lm loss value: 1.309309E+00 | lm loss PPL: 3.703612E+00 | \n",
      "-------------------------------------------------------------------------------------------------\n",
      " iteration    47500/  125000 | consumed samples:       380000 | elapsed time per iteration (ms): 4005.2 | learning rate: 1.434E-04 | global batch size:     8 | lm loss: 6.389265E-01 | loss scale: 131072.0 | grad norm: 0.095 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1171.07 | backward-compute: 2769.60 | backward-params-all-reduce: 22.18 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 5.35 | optimizer-clip-main-grad: 4.25 | optimizer-copy-main-to-model-params: 5.74 | optimizer: 38.60 | batch-generator: 2.34\n",
      " iteration    48000/  125000 | consumed samples:       384000 | elapsed time per iteration (ms): 3983.5 | learning rate: 1.432E-04 | global batch size:     8 | lm loss: 6.383853E-01 | loss scale: 131072.0 | grad norm: 0.110 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1149.34 | backward-compute: 2769.64 | backward-params-all-reduce: 22.21 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 5.34 | optimizer-clip-main-grad: 4.26 | optimizer-copy-main-to-model-params: 5.74 | optimizer: 38.59 | batch-generator: 2.20\n",
      "-------------------------------------------------------------------------------------------------\n",
      " validation loss at iteration 48000 | lm loss value: 1.341305E+00 | lm loss PPL: 3.824031E+00 | \n",
      "-------------------------------------------------------------------------------------------------\n",
      " iteration    48500/  125000 | consumed samples:       388000 | elapsed time per iteration (ms): 4004.4 | learning rate: 1.431E-04 | global batch size:     8 | lm loss: 6.352239E-01 | loss scale: 262144.0 | grad norm: 0.105 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1170.75 | backward-compute: 2769.78 | backward-params-all-reduce: 21.75 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 5.34 | optimizer-clip-main-grad: 4.25 | optimizer-copy-main-to-model-params: 5.62 | optimizer: 38.46 | batch-generator: 2.26\n",
      " iteration    49000/  125000 | consumed samples:       392000 | elapsed time per iteration (ms): 3983.7 | learning rate: 1.429E-04 | global batch size:     8 | lm loss: 6.378376E-01 | loss scale: 262144.0 | grad norm: 0.103 | number of skipped iterations:   1 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1149.84 | backward-compute: 2769.72 | backward-params-all-reduce: 21.89 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 5.34 | optimizer-clip-main-grad: 4.24 | optimizer-copy-main-to-model-params: 5.72 | optimizer: 38.51 | batch-generator: 2.28\n",
      "-------------------------------------------------------------------------------------------------\n",
      " validation loss at iteration 49000 | lm loss value: 1.303956E+00 | lm loss PPL: 3.683840E+00 | \n",
      "-------------------------------------------------------------------------------------------------\n",
      " iteration    49500/  125000 | consumed samples:       396000 | elapsed time per iteration (ms): 4004.0 | learning rate: 1.428E-04 | global batch size:     8 | lm loss: 6.390486E-01 | loss scale: 32768.0 | grad norm: 0.110 | number of skipped iterations:   3 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1170.71 | backward-compute: 2769.34 | backward-params-all-reduce: 21.86 | backward-embedding-all-reduce: 0.02 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 5.34 | optimizer-clip-main-grad: 4.21 | optimizer-copy-main-to-model-params: 5.70 | optimizer: 38.39 | batch-generator: 2.22\n",
      " iteration    50000/  125000 | consumed samples:       400000 | elapsed time per iteration (ms): 3982.3 | learning rate: 1.426E-04 | global batch size:     8 | lm loss: 6.349749E-01 | loss scale: 32768.0 | grad norm: 0.096 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1149.51 | backward-compute: 2768.75 | backward-params-all-reduce: 21.94 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.17 | optimizer-unscale-and-check-inf: 5.34 | optimizer-clip-main-grad: 4.25 | optimizer-copy-main-to-model-params: 5.62 | optimizer: 38.46 | batch-generator: 2.32\n",
      "-------------------------------------------------------------------------------------------------\n",
      " validation loss at iteration 50000 | lm loss value: 1.305640E+00 | lm loss PPL: 3.690051E+00 | \n",
      "-------------------------------------------------------------------------------------------------\n",
      "saving checkpoint at iteration   50000 to checkpoints/gpt_creditcard\n",
      "  successfully saved checkpoint at iteration   50000 to checkpoints/gpt_creditcard\n",
      "time (ms) | save-checkpoint: 11546.25\n",
      " iteration    50500/  125000 | consumed samples:       404000 | elapsed time per iteration (ms): 4026.7 | learning rate: 1.425E-04 | global batch size:     8 | lm loss: 6.369180E-01 | loss scale: 65536.0 | grad norm: 0.102 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1170.70 | backward-compute: 2768.83 | backward-params-all-reduce: 21.84 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 5.33 | optimizer-clip-main-grad: 4.23 | optimizer-copy-main-to-model-params: 5.75 | optimizer: 38.54 | batch-generator: 2.28\n",
      " iteration    51000/  125000 | consumed samples:       408000 | elapsed time per iteration (ms): 3983.0 | learning rate: 1.423E-04 | global batch size:     8 | lm loss: 6.342582E-01 | loss scale: 65536.0 | grad norm: 0.108 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1149.81 | backward-compute: 2769.14 | backward-params-all-reduce: 21.86 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 5.34 | optimizer-clip-main-grad: 4.23 | optimizer-copy-main-to-model-params: 5.73 | optimizer: 38.53 | batch-generator: 2.20\n",
      "-------------------------------------------------------------------------------------------------\n",
      " validation loss at iteration 51000 | lm loss value: 1.309753E+00 | lm loss PPL: 3.705260E+00 | \n",
      "-------------------------------------------------------------------------------------------------\n",
      " iteration    51500/  125000 | consumed samples:       412000 | elapsed time per iteration (ms): 4003.2 | learning rate: 1.421E-04 | global batch size:     8 | lm loss: 6.353171E-01 | loss scale: 131072.0 | grad norm: 0.100 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1170.01 | backward-compute: 2769.16 | backward-params-all-reduce: 21.85 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.17 | optimizer-unscale-and-check-inf: 5.34 | optimizer-clip-main-grad: 4.25 | optimizer-copy-main-to-model-params: 5.62 | optimizer: 38.47 | batch-generator: 2.30\n",
      " iteration    52000/  125000 | consumed samples:       416000 | elapsed time per iteration (ms): 3983.2 | learning rate: 1.420E-04 | global batch size:     8 | lm loss: 6.358087E-01 | loss scale: 131072.0 | grad norm: 0.088 | number of skipped iterations:   0 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1149.67 | backward-compute: 2769.44 | backward-params-all-reduce: 21.79 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.16 | optimizer-unscale-and-check-inf: 5.34 | optimizer-clip-main-grad: 4.25 | optimizer-copy-main-to-model-params: 5.73 | optimizer: 38.56 | batch-generator: 2.24\n",
      "-------------------------------------------------------------------------------------------------\n",
      " validation loss at iteration 52000 | lm loss value: 1.322008E+00 | lm loss PPL: 3.750947E+00 | \n",
      "-------------------------------------------------------------------------------------------------\n",
      " iteration    52500/  125000 | consumed samples:       420000 | elapsed time per iteration (ms): 4004.3 | learning rate: 1.418E-04 | global batch size:     8 | lm loss: 6.359784E-01 | loss scale: 262144.0 | grad norm: 0.097 | number of skipped iterations:   1 | number of nan iterations:   0 |\n",
      "time (ms) | forward-compute: 1170.71 | backward-compute: 2769.48 | backward-params-all-reduce: 21.86 | backward-embedding-all-reduce: 0.03 | optimizer-copy-to-main-grad: 5.17 | optimizer-unscale-and-check-inf: 5.34 | optimizer-clip-main-grad: 4.24 | optimizer-copy-main-to-model-params: 5.73 | optimizer: 38.53 | batch-generator: 2.30\n",
      "^C\n",
      "WARNING:torch.distributed.elastic.agent.server.api:Received 2 death signal, shutting down workers\n",
      "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 18643 closing signal SIGINT\n",
      "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 18644 closing signal SIGINT\n",
      "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 18645 closing signal SIGINT\n",
      "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 18646 closing signal SIGINT\n",
      "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 18647 closing signal SIGINT\n",
      "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 18648 closing signal SIGINT\n",
      "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 18649 closing signal SIGINT\n",
      "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 18650 closing signal SIGINT\n"
     ]
    }
   ],
   "source": [
    "!./pretrain_step.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please shut down the Kernel\n",
    "\n",
    "Ex. `Kernel -> Shut down kernel`, or in Jupyter Lab, navigating to the `Running Terminals and Kernels` Tab on the left sidebar, highlighting the mouse over this notebook's name in the `KERNELS` Section and selecting the `X` that appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
