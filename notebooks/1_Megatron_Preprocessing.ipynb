{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "############################################################################\n",
    "##\n",
    "## Copyright (C) 2022 NVIDIA Corporation.  All rights reserved.\n",
    "##\n",
    "## NVIDIA Sample Code\n",
    "##\n",
    "## Please refer to the NVIDIA end user license agreement (EULA) associated\n",
    "## with this source code for terms and conditions that govern your use of\n",
    "## this software. Any use, reproduction, disclosure, or distribution of\n",
    "## this software and related documentation outside the terms of the EULA\n",
    "## is strictly prohibited.\n",
    "##\n",
    "############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "# 1. Introduction to Megatron-LM Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a href=\"https://github.com/NVIDIA/Megatron-LM\">Megatron</a> [[1](#1_1),[2](#1_2)] is a large, powerful transformer developed by the [Applied Deep Learning Research team at NVIDIA](https://nv-adlr.github.io/). The repository is for ongoing research on training large transformer language models at scale. We developed efficient, model-parallel (tensor and pipeline), and multi-node pre-training of transformer based models such as GPT, BERT, and T5 using mixed precision.\n",
    "\n",
    "Below are some of the projects where we have directly used Megatron:\n",
    "    \n",
    "   - FinMegatron: Large Financial Domain Language Models\n",
    "   - BioMegatron: Larger Biomedical Domain Language Model\n",
    "   - End-to-End Training of Neural Retrievers for Open-Domain Question Answering\n",
    "   - Large Scale Multi-Actor Generative Dialog Modeling\n",
    "   - Local Knowledge Powered Conversational Agents\n",
    "   - MEGATRON-CNTRL: Controllable Story Generation with External Knowledge Using Large-Scale Language Models\n",
    "   - RACE Reading Comprehension Dataset Leaderboard\n",
    "   - Scaling Language Model Training to a Trillion Parameters Using Megatron\n",
    "   - Training Question Answering Models From Synthetic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "Megatron-LM implements <u>pipeline model parallelism</u> and <u>tensor model parallelism</u> to efficiently distribute workloads across multiple GPUs and multiple nodes. \n",
    "\n",
    "- Pipeline Parallelism splits layers across multiple GPUs\n",
    "- Tensor Parallelism splits individual layers across multiple GPUs\n",
    "\n",
    "Our Megatron-LM framework will handle this for you using `tensor-model-parallel-size` and `pipeline-model-parallel-size` parameters found in the `pretrain_step.sh` script, which we will use to train our model. Our model config file has `TENSOR_MP_SIZE` and `PIPELINE_MP_SIZE` variables, which are passed to the `pretrain_step.sh` script for convenience.\n",
    "\n",
    "\n",
    "If you are familiar with the <a href=\"https://pytorch.org/\">PyTorch</a> Framework, you can move parts of a Neural Network model to different GPUs using `layer.to(device)`. This approach of moving different parts of a model to different GPUs lets the data scientist implement pipeline parallelism by hand. \n",
    "\n",
    "\n",
    "<br>\n",
    "<center><img src=images/model_parallelism.png width=\"40%\" height=\"40%\" style=\"display=block; margin:auto\" alt=\"model parallelism\"/></center>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "<div><font size=\"4\">While the idea of pipeline parallelism, moving neural network layers to different devices, makes intuitive sense, let's explore the idea behind Tensor Parallelism further by example. Consider the following matrix multiplication taking place on a single GPU 0 using input values and the first layer in a neural network:</font><div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "<center><img src=images/tensor_parallelism_whole.png width=\"40%\" height=\"40%\" style=\"display=block; margin:auto\" alt=\"matrix multiplication for tensor parallelism\"/></center>\n",
    "\n",
    "<div><font size=\"4\">Notice that we can split this matrix multiplication across multiple GPUs by splitting the matrices as follows:</font><div>\n",
    "\n",
    "\n",
    "<center><img src=images/tensor_parallelism_split.png width=\"40%\" height=\"40%\" style=\"display=block; margin:auto\" alt=\"matrix multiplication for tensor parallelism distributed across two GPUs\"/></center>\n",
    "\n",
    "<div><font size=\"4\">The values are the same! Thus, tensor parallelism becomes useful especially for very large Deep Learning models where <u>it may be impossible to fit an entire layer on a single GPU, or where synchronization is important.</u></font><div> \n",
    "\n",
    "\n",
    "<div><font size=\"4\">When we are training our Deep learning model, data must pass from the beginning of the network through to the end, and then weights backpropagated to the beginning. For very large deep learning models, communication across the multiple GPUs can become a bottleneck. The image below shows that we get a `bubble` effect, `shown in grey` when there is no communication for 4 GPUS. In the bottom image, Megatron handles this by creating multiple virtual stages to reduce the communication bottleneck and reduce training time.</font><div>\n",
    "\n",
    "<center><img src=images/interleaving.png width=\"40%\" height=\"40%\" style=\"display=block; margin:auto\" alt=\"reducing communication in multi-GPU training with interleaving\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Megatron is also used in <a href=\"https://developer.nvidia.com/nvidia-nemo#nemo-megatron\">NeMo Megatron</a>, a framework to help enterprises overcome the challenges of building and training sophisticated natural language processing models with billions and trillions of parameters.\n",
    "\n",
    "Our codebase is capable of efficiently training very large (hundreds of billions of parameters) language models with both model and data parallelism. To demonstrate how the code scales with multiple GPUs and model sizes, we consider GPT models from 1 billion all the way to 1 trillion parameters. All models use a vocabulary size of 51,200 and a sequence length of 2048. \n",
    "\n",
    "We vary <u>hidden size, number of attention heads, and number of layers to arrive at a specific model size.</u> As the model size increases, we also modestly increase the batch size. \n",
    "\n",
    "We leverage NVIDIA's Selene supercomputer to perform scaling studies and use up to 3072 A100 GPUs for the largest model. The table below shows the model configurations along with the achieved FLOPs (both per GPU and aggregate over all GPUs). Note that these results are from benchmark runs and these models were not trained to convergence; however, the FLOPs are measured for end-to-end training, i.e., includes all operations including data loading, optimization, and even logging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": [
     "cell.metadata.heading_collapsed",
     "=",
     "true"
    ]
   },
   "source": [
    "# 2. Scaling Megatron up to a Supercomputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "The Megatron-LM model framework has been used to train small and very large models up to and using NVIDIA Selene, currently <a href=\"https://www.top500.org/system/179842/\">6th on the Top 500</a> list of Supercomputers as of November 2021.  We will go over the `pretrain_step.sh` script at the end of this notebook, which will detail how to adjust model size.\n",
    "\n",
    "Large supercomputers can have bottlenecks in either the compute, storage, or networking. If one of these components is slow, it can cause an application to slow down. For very large NLP models trained on GPUs, a common bottleneck is in the network and storage because the training data needs to be read from disk and transferred over the network to the feed GPUs. In addition, large models may span multiple GPUs or nodes, which requires sending/receiving model parameters. <a href=\"https://www.nvidia.com/en-us/data-center/dgx-superpod/\">NVIDIA DGX SuperPOD™</a> is an AI data center infrastructure platform that enables IT to deliver performance—without compromise—for every user and workload. DGX SuperPOD offers leadership-class accelerated infrastructure and agile, scalable performance for the most challenging AI and high-performance computing (HPC) workloads, with industry-proven results. DGX SuperPOD uses InfiniBand networking to achieve best in class performance and includes advantages such as in-network compute, which can speed up training times.\n",
    "\n",
    "<center><img src=\"images/cases_april2021.png\" width=\"50%\" height=\"50%\" style=\"display=block; margin:auto\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "# 3. Train GPT on multi-node multiple GPUs\n",
    "\n",
    "Training using the Megatron Framework involves 4 steps:\n",
    "\n",
    "<center><img src=\"images/E2E_Megatron_steps.png\" width=\"50%\" height=\"50%\" style=\"display=block; margin:auto\"/></center>\n",
    "\n",
    "<div><font size=\"4\">In the rest of the notebook, we will focus on  <strong>(1) data preprocessing of credit card transactions</strong></font><div></br>\n",
    "\n",
    "\n",
    "Megatron expects training data in a <em>json lines</em> format and is commonly denoted with a JSON object per line. By default, Megatron expects the `text` field to contain the document (in this case, transactions) of interest.\n",
    "\n",
    "```json\n",
    "{\"text\": \n",
    " \"791,1,68.00,2018-01-02 09:10:00,2018,1,2,9,10,Swipe,Transaction,12345536,New,York,NY,10017,8005<NA>,0\\n\"\n",
    " }\n",
    "```\n",
    "\n",
    "We can model temporal information by placing continuous sets of credit card transactions in the `text` field. Megatron defines a special `<|endoftext|>` token to denote the beginning and end of a document. The attention mask will stop at the boundary of the `<|endoftext|>` token so no attention is applied across this token. As we just learned, to model temporal information in the time series, we want to make sure that the `<|endoftext|>` is added between continuous sections. We'll come back to how to model this on the credit card dataset after we have tokenized our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "# 4. Tokenizing text\n",
    "\n",
    "This link contains a <a href=\"https://huggingface.co/docs/transformers/tokenizer_summary\">summary of common tokenizers</a> used in GPT, BERT, or other models. GPT2 uses the BPE tokenizer:\n",
    "\n",
    "\n",
    "BPE tokenizer stands for Byte Pair Encoding Tokenizer and is used in GPT2. Here is the GPT2 paper\n",
    "<a href=\"https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf\">GPT2 paper</a> and the \n",
    "<a href=https://arxiv.org/pdf/1508.07909.pdf>BPE tokenizer paper</a>, which describe the tokenization process in further detail. \n",
    "\n",
    "In the section below, we will explore the BPE tokenizer, explain its benefits, and drawbacks for tabular data tokenization. Then we will implement a custom `TabularTokenizer` class, which will attempt to address some issues with BPE tokenization for tabular data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "## 4.1 Original BPE Tokenizer\n",
    "\n",
    "Overview:\n",
    "- Frequent pairs of characters iteratively merged together to construct a model vocabulary\n",
    "- Hyperparameter: number of merge iterations\n",
    "\n",
    "\n",
    "The motivation for the BPE tokenizer is to create a word segmentation algorithm where frequent pairs of characters are merged together. For example, the pair `('A', 'B')` can be merged to the symbol `('AB')`. Frequent pairs can also be merged together, eventually yielding whole words. The number of merge operations is the only hyperparameter of the algorithm.\n",
    "\n",
    "From the paper, the code to implement BPE in Python is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Merging Pair: ('e', 's')\n",
      "Vocab: {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w es t ? </w>': 6, 'w i d es t ? </w>': 3}\n",
      "\n",
      "Iteration: 2\n",
      "Merging Pair: ('es', 't')\n",
      "Vocab: {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w est ? </w>': 6, 'w i d est ? </w>': 3}\n",
      "\n",
      "Iteration: 3\n",
      "Merging Pair: ('est', '?')\n",
      "Vocab: {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w est? </w>': 6, 'w i d est? </w>': 3}\n",
      "\n",
      "Iteration: 4\n",
      "Merging Pair: ('est?', '</w>')\n",
      "Vocab: {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w est?</w>': 6, 'w i d est?</w>': 3}\n",
      "\n",
      "Iteration: 5\n",
      "Merging Pair: ('l', 'o')\n",
      "Vocab: {'lo w </w>': 5, 'lo w e r </w>': 2, 'n e w est?</w>': 6, 'w i d est?</w>': 3}\n",
      "\n",
      "Iteration: 6\n",
      "Merging Pair: ('lo', 'w')\n",
      "Vocab: {'low </w>': 5, 'low e r </w>': 2, 'n e w est?</w>': 6, 'w i d est?</w>': 3}\n",
      "\n",
      "Iteration: 7\n",
      "Merging Pair: ('n', 'e')\n",
      "Vocab: {'low </w>': 5, 'low e r </w>': 2, 'ne w est?</w>': 6, 'w i d est?</w>': 3}\n",
      "\n",
      "Iteration: 8\n",
      "Merging Pair: ('ne', 'w')\n",
      "Vocab: {'low </w>': 5, 'low e r </w>': 2, 'new est?</w>': 6, 'w i d est?</w>': 3}\n",
      "\n",
      "Iteration: 9\n",
      "Merging Pair: ('new', 'est?</w>')\n",
      "Vocab: {'low </w>': 5, 'low e r </w>': 2, 'newest?</w>': 6, 'w i d est?</w>': 3}\n",
      "\n",
      "Iteration: 10\n",
      "Merging Pair: ('low', '</w>')\n",
      "Vocab: {'low</w>': 5, 'low e r </w>': 2, 'newest?</w>': 6, 'w i d est?</w>': 3}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "BPE algorithm from Neural Machine Translation of Rare Words with Subword Units\n",
    "https://arxiv.org/pdf/1508.07909.pdf\n",
    "\"\"\"\n",
    "\n",
    "import re, collections\n",
    "\n",
    "def get_stats(vocab):\n",
    "    pairs = collections.defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols)-1):\n",
    "            pairs[symbols[i],symbols[i+1]] += freq\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, v_in):\n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    for word in v_in:\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "    return v_out\n",
    "\n",
    "vocab = {'l o w </w>' : 5, 'l o w e r </w>' : 2,\n",
    "         'n e w e s t ? </w>':6, 'w i d e s t ? </w>':3}\n",
    "\n",
    "NUM_MERGES = 10  # HYPERPARAMETER\n",
    "\n",
    "for i in range(1, NUM_MERGES+1):\n",
    "    pairs = get_stats(vocab)\n",
    "    best = max(pairs, key=pairs.get)\n",
    "    vocab = merge_vocab(best, vocab)\n",
    "#     print(f'Iteration: {i}\\nPairs: {pairs}\\nMerging Pair: {best}\\nVocab: {vocab}\\n')\n",
    "    print(f'Iteration: {i}\\nMerging Pair: {best}\\nVocab: {vocab}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "Play around with the value of `NUM_MERGES`.\n",
    "As `NUM_MERGES` increases the `vocab` has a greater number of full words (no spaces). Think about what happens if we set `NUM_MERGES` to a large value, example `NUM_MERGES > 15`?\n",
    "\n",
    "We see from adjusting `NUM_MERGES` we can tune our vocabulary to have more discrete letters when `NUM_MERGES` is a small value and more full words when `NUM_MERGES` is a larger value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "## 4.2 GPT2 BPE optimizations over original implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "- no merging on punctuation, with exception for spaces \n",
    "\n",
    "The authors of GPT2, in their paper, noted that the BPE tokenizer produced versions of words with punctuation together like `dog.`, `dog?`, and `dog!`. We obtained the same analogous result in the third merge iteration of above, `est` was merged with `?` to make the symbol `est?`. As a result, the model vocabulary was not optimally being used in the original BPE tokenizer implementation. The GPT2 authors prevented punctuation, except for spaces, from being used in merging. This approach still allows for generality like the original BPE tokenizer, while allowing additional tokens to be used for the same amount of space as before.\n",
    "\n",
    "The full GPT2 BPE tokenizer implementation is located [here](./megatron/tokenizer/gpt2_tokenization.py).\n",
    "\n",
    "### Tokenization of Tabular Data\n",
    "\n",
    "A tokenizer, when applied to a tabular corpus should incorporate the structural information in each column of the table. As we learned above, the GPT2 BPE tokenizer will merge tokens together based on the frequency of occurrence. Thus, for a given column, the tokenizer could yield a different number of tokens, producing different numbers of tokens on any two rows. \n",
    "\n",
    "For float columns, the structural information of the float numbers can get lost if an NLP tokenizer is used. \n",
    "\n",
    "For example, we could break down the float `19.2` using the tokens `1`, `9.` and `2` and merge these together to form `19.2` again, but the tokens have lost their meaning, which is that `1` is a tens digit, 9 is a ones digit, and 2 is located in the first position after the decimal point. \n",
    "\n",
    "Moreover, in the case of columns with a few different values, this is a wasteful use of the vocabulary as we can simply `factorize` those columns. For example, take a look at the `Use Chip` column, \n",
    "\n",
    "```python\n",
    "df['Use Chip'].value_counts().to_pandas().to_dict()\n",
    "\n",
    "Out:\n",
    "{'Swipe Transaction': 15386082,\n",
    " 'Chip Transaction': 6287598,\n",
    " 'Online Transaction': 2713220}\n",
    "```\n",
    "\n",
    "and computing like before - only showing the last iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 10\n",
      "Pairs: defaultdict(<class 'int'>, {('S', 'w'): 15386082, ('w', 'i'): 15386082, ('i', 'p'): 21673680, ('p', 'e'): 15386082, ('e', 'Transactio'): 18099302, ('Transactio', 'n'): 24386900, ('n', '</w>'): 24386900, ('C', 'h'): 6287598, ('h', 'i'): 6287598, ('p', 'Transactio'): 6287598, ('O', 'n'): 2713220, ('n', 'l'): 2713220, ('l', 'i'): 2713220, ('i', 'n'): 2713220, ('n', 'e'): 2713220})\n",
      "Merging Pair: ('Transactio', 'n')\n",
      "Vocab: {'S w i p e   Transaction </w>': 15386082, 'C h i p   Transaction </w>': 6287598, 'O n l i n e   Transaction </w>': 2713220}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vocab = {'S w i p e   T r a n s a c t i o n </w>': 15386082,\n",
    "         'C h i p   T r a n s a c t i o n </w>': 6287598,\n",
    "         'O n l i n e   T r a n s a c t i o n </w>': 2713220}\n",
    "\n",
    "NUM_MERGES = 10\n",
    "\n",
    "for i in range(1, NUM_MERGES+1):\n",
    "    pairs = get_stats(vocab)\n",
    "    best = max(pairs, key=pairs.get)\n",
    "    vocab = merge_vocab(best, vocab)\n",
    "print(f'Iteration: {i}\\nPairs: {pairs}\\nMerging Pair: {best}\\nVocab: {vocab}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "After 10 merge iterations, the method did not produce the only 3 tokens needed for this column.\n",
    "\n",
    "## 4.3 So what alternative tokenization options exist for Tabular data?\n",
    "\n",
    "As shown in the [TabFormer paper](https://arxiv.org/abs/2011.01843) where this dataset is derived, the authors constructed a tabular tokenizer that considers the structural information in the table, and <strong>uses the same number of tokens (1) per column</strong> and is akin to LabelEncoding, or factorization. However, since TabFormer uses a single token for each of the columns, it can cause either accuracy loss if the number of tokens is small for the column, or weak generalization if the number of tokens is too large. We improve it by using multiple tokens to code the columns. For example, the floating number \"134.09\" can be tokenized into multiple integers. \n",
    "\n",
    "Those familiar with `quantization` methods may be familiar with how the Real Numbers can be \"digitized\" to discrete integers, at the loss of a little accuracy. The most popular quantization method is rounding a number up or down to the nearest integer. Another density-based approach is to calculate a histogram of floating numbers and use the bin number as a representation of a number - so we could represent the number `2.71828` given the following bin edges: `[0,1), [1,4), [4,10), [10,12)]` as the number `1` since it occurs in bin 1 (bin 0 is the first bin). \n",
    "\n",
    "We implemented our own custom encoder/decoder used in the tokenizer. It uses FloatTokenizer for the `Amount` column, and a categorical tokenizer for the rest of the columns. The trained encoder and decoder are saved to a file and can be used later. We choose to use 4 tokens to encode the floating numbers in this example. Using more tokens will yield a more accurate reverse-lookup.\n",
    "\n",
    "Briefly, the method we implemented for tokenizing floating point numbers is the following:\n",
    "\n",
    "- Set minimum value = 0 by subtracting from the column\n",
    "- Compress these values by taking `log1p` \n",
    "- Compute the max digits of these log values using base 10 logarithm, `log10`\n",
    "- Allow for extra user-defined precision using the `extra_digits = code_len - max_digits` where `code_len` is user defined and `max_digits` was calculated in the prior step\n",
    "- Compute and save a lookup and reverse-lookup table for the digits up the precision defined by `extra_digits`\n",
    "\n",
    "    \n",
    "So if the `extra_digits` is 4 and a logarithmed value is: `1.23456`, we would truncate after the `4`\n",
    "    \n",
    "Computing the reverse transform follows the reverse steps, namely compute\n",
    "``` python \n",
    "reverse_transformed_value = np.exp(v / 10**self.extra_digits) + min_value - 1.0\n",
    "```\n",
    "\n",
    "<strong>This link will bring you to the [code for our custom tabular tokenizer](./coder/column_code.py) implementation.</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's tokenize the credit card dataset that we saw in the previous notebook. We use the NVIDIA RAPIDS library to read in and parse much of the data. For this dataset (24M rows), we can achieve speedups using GPUs for parallelism in this preprocessing scheme. RAPIDS has GPU-enabled libraries that are analogous to the CPU counterpart. For example, the `pandas` equivalent is `cudf` and the `scikit-learn` equivalent is `cuml`.\n",
    "\n",
    "We'll use `cudf` to read and preprocess the data, and numpy and sklearn where certain functionality is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "# 5. ETL with cuDF and Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# we'll save the \n",
    "data_fp = './data/card_transaction.v1.parquet'\n",
    "out_data_fp = './data/card_transactions_fixed.parquet'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## cuDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed (s):  1.289\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>card</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>amount</th>\n",
       "      <th>use chip</th>\n",
       "      <th>merchant name</th>\n",
       "      <th>merchant city</th>\n",
       "      <th>merchant state</th>\n",
       "      <th>zip</th>\n",
       "      <th>mcc</th>\n",
       "      <th>errors</th>\n",
       "      <th>is_fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23080265</th>\n",
       "      <td>888</td>\n",
       "      <td>1</td>\n",
       "      <td>2019</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>59.15</td>\n",
       "      <td>Chip Transaction</td>\n",
       "      <td>-245178307025547046</td>\n",
       "      <td>Woodbury Heights</td>\n",
       "      <td>NJ</td>\n",
       "      <td>08097</td>\n",
       "      <td>5311</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7068011</th>\n",
       "      <td>1008</td>\n",
       "      <td>5</td>\n",
       "      <td>2009</td>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>16.58</td>\n",
       "      <td>Swipe Transaction</td>\n",
       "      <td>6135208568923449408</td>\n",
       "      <td>Matthews</td>\n",
       "      <td>NC</td>\n",
       "      <td>28105</td>\n",
       "      <td>9402</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14681442</th>\n",
       "      <td>98</td>\n",
       "      <td>4</td>\n",
       "      <td>2014</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>44</td>\n",
       "      <td>4.10</td>\n",
       "      <td>Swipe Transaction</td>\n",
       "      <td>5428932742579976001</td>\n",
       "      <td>Hollywood</td>\n",
       "      <td>FL</td>\n",
       "      <td>33024</td>\n",
       "      <td>5921</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995083</th>\n",
       "      <td>1649</td>\n",
       "      <td>1</td>\n",
       "      <td>2004</td>\n",
       "      <td>9</td>\n",
       "      <td>21</td>\n",
       "      <td>10</td>\n",
       "      <td>28</td>\n",
       "      <td>204.56</td>\n",
       "      <td>Swipe Transaction</td>\n",
       "      <td>6299212792632776248</td>\n",
       "      <td>Blair</td>\n",
       "      <td>NE</td>\n",
       "      <td>68008</td>\n",
       "      <td>8931</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16432346</th>\n",
       "      <td>485</td>\n",
       "      <td>0</td>\n",
       "      <td>2015</td>\n",
       "      <td>7</td>\n",
       "      <td>26</td>\n",
       "      <td>16</td>\n",
       "      <td>46</td>\n",
       "      <td>83.30</td>\n",
       "      <td>Chip Transaction</td>\n",
       "      <td>-9172319837809732852</td>\n",
       "      <td>League City</td>\n",
       "      <td>TX</td>\n",
       "      <td>77573</td>\n",
       "      <td>7538</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          user  card  year  month  day  hour  minute  amount  \\\n",
       "23080265   888     1  2019      6    9    15      11   59.15   \n",
       "7068011   1008     5  2009      9   16    16       5   16.58   \n",
       "14681442    98     4  2014      7   13     8      44    4.10   \n",
       "1995083   1649     1  2004      9   21    10      28  204.56   \n",
       "16432346   485     0  2015      7   26    16      46   83.30   \n",
       "\n",
       "                   use chip        merchant name     merchant city  \\\n",
       "23080265   Chip Transaction  -245178307025547046  Woodbury Heights   \n",
       "7068011   Swipe Transaction  6135208568923449408          Matthews   \n",
       "14681442  Swipe Transaction  5428932742579976001         Hollywood   \n",
       "1995083   Swipe Transaction  6299212792632776248             Blair   \n",
       "16432346   Chip Transaction -9172319837809732852       League City   \n",
       "\n",
       "         merchant state    zip   mcc errors  is_fraud  \n",
       "23080265             NJ  08097  5311   <NA>         0  \n",
       "7068011              NC  28105  9402   <NA>         0  \n",
       "14681442             FL  33024  5921   <NA>         0  \n",
       "1995083              NE  68008  8931   <NA>         0  \n",
       "16432346             TX  77573  7538   <NA>         0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cudf\n",
    "import time\n",
    "\n",
    "start =time.time()\n",
    "\n",
    "df = cudf.read_parquet(data_fp)\n",
    "\n",
    "# remove the dollar string and convert amount to float\n",
    "df['Amount'] = df['Amount'].str.replace('$', '').astype('float').round(2)\n",
    "# fix the zip column\n",
    "df['Zip'] = df['Zip'].astype(int).astype(str).str.zfill(5)\n",
    "# strip some leading spaces present in the Merchant City column\n",
    "df['Merchant City'] = df['Merchant City'].str.strip()#.str.replace('/','') #Naval Air Station/ Jrb\n",
    "\n",
    "# parse the time into minute and second\n",
    "df['Hour'] = df['Time'].str[0:2].astype(int)\n",
    "df['Minute'] = df['Time'].str[-2:].astype(int)\n",
    "# remove the 'Time' Column once it is parsed\n",
    "del df['Time']\n",
    "\n",
    "# rename and lowercase columns\n",
    "df = df.rename(columns={'Errors?': 'errors',\n",
    "                        'Is Fraud?': 'is_fraud'\n",
    "                        })\n",
    "df.columns = [i.lower() for i in df.columns]\n",
    "\n",
    "df['is_fraud'], fraud_key = df['is_fraud'].factorize()\n",
    "df[['user', 'card', 'year', 'month', 'day', 'hour', 'minute', 'merchant name', 'mcc', 'is_fraud']] = df[['user', 'card', 'year', 'month', 'day', 'hour', 'minute', 'merchant name', 'mcc', 'is_fraud']].astype('int64')\n",
    "\n",
    "# sort rows by date and re-order the cols\n",
    "df = df.sort_values(['year', 'month', 'day', 'hour', 'minute'])[['user', 'card', 'year', 'month', 'day', 'hour', 'minute', 'amount',\n",
    "                             'use chip', 'merchant name', 'merchant city', 'merchant state', 'zip',\n",
    "                             'mcc', 'errors', 'is_fraud']].reset_index(drop=True)\n",
    "\n",
    "for col, typ in zip(df.columns, df.dtypes):\n",
    "    # print(co, t, t=='object')\n",
    "    if typ=='object':\n",
    "        df[col] = df[col].str.strip()\n",
    "        \n",
    "        \n",
    "print(f'Elapsed (s): {time.time() - start: 0.3f}')\n",
    "df.to_parquet(out_data_fp)\n",
    "df.sample(5, random_state=2718)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = cudf.read_parquet(out_data_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>card</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>amount</th>\n",
       "      <th>use chip</th>\n",
       "      <th>merchant name</th>\n",
       "      <th>merchant city</th>\n",
       "      <th>merchant state</th>\n",
       "      <th>zip</th>\n",
       "      <th>mcc</th>\n",
       "      <th>errors</th>\n",
       "      <th>is_fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>791</td>\n",
       "      <td>1</td>\n",
       "      <td>1991</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>68.00</td>\n",
       "      <td>Swipe Transaction</td>\n",
       "      <td>2027553650310142703</td>\n",
       "      <td>Burke</td>\n",
       "      <td>VA</td>\n",
       "      <td>22015</td>\n",
       "      <td>5541</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>791</td>\n",
       "      <td>1</td>\n",
       "      <td>1991</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "      <td>-68.00</td>\n",
       "      <td>Swipe Transaction</td>\n",
       "      <td>2027553650310142703</td>\n",
       "      <td>Burke</td>\n",
       "      <td>VA</td>\n",
       "      <td>22015</td>\n",
       "      <td>5541</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>791</td>\n",
       "      <td>1</td>\n",
       "      <td>1991</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>21</td>\n",
       "      <td>113.62</td>\n",
       "      <td>Swipe Transaction</td>\n",
       "      <td>2027553650310142703</td>\n",
       "      <td>Burke</td>\n",
       "      <td>VA</td>\n",
       "      <td>22015</td>\n",
       "      <td>5541</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>791</td>\n",
       "      <td>1</td>\n",
       "      <td>1991</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>30</td>\n",
       "      <td>114.73</td>\n",
       "      <td>Swipe Transaction</td>\n",
       "      <td>-7269691894846892021</td>\n",
       "      <td>Burke</td>\n",
       "      <td>VA</td>\n",
       "      <td>22015</td>\n",
       "      <td>5411</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>791</td>\n",
       "      <td>1</td>\n",
       "      <td>1991</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>251.71</td>\n",
       "      <td>Swipe Transaction</td>\n",
       "      <td>-3693650930986299431</td>\n",
       "      <td>Burke</td>\n",
       "      <td>VA</td>\n",
       "      <td>22015</td>\n",
       "      <td>4814</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24386895</th>\n",
       "      <td>1659</td>\n",
       "      <td>2</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>23</td>\n",
       "      <td>51</td>\n",
       "      <td>7.67</td>\n",
       "      <td>Chip Transaction</td>\n",
       "      <td>7231700044622779845</td>\n",
       "      <td>Cosby</td>\n",
       "      <td>TN</td>\n",
       "      <td>37722</td>\n",
       "      <td>5300</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24386896</th>\n",
       "      <td>863</td>\n",
       "      <td>1</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>23</td>\n",
       "      <td>53</td>\n",
       "      <td>49.06</td>\n",
       "      <td>Chip Transaction</td>\n",
       "      <td>7654254764356253071</td>\n",
       "      <td>North Brunswick</td>\n",
       "      <td>NJ</td>\n",
       "      <td>08902</td>\n",
       "      <td>5912</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24386897</th>\n",
       "      <td>1300</td>\n",
       "      <td>0</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>23</td>\n",
       "      <td>56</td>\n",
       "      <td>51.29</td>\n",
       "      <td>Online Transaction</td>\n",
       "      <td>-6458444334611773637</td>\n",
       "      <td>ONLINE</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>4784</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24386898</th>\n",
       "      <td>1366</td>\n",
       "      <td>2</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>23</td>\n",
       "      <td>56</td>\n",
       "      <td>132.73</td>\n",
       "      <td>Chip Transaction</td>\n",
       "      <td>-7398558035733466800</td>\n",
       "      <td>Rockville Centre</td>\n",
       "      <td>NY</td>\n",
       "      <td>11570</td>\n",
       "      <td>5812</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24386899</th>\n",
       "      <td>446</td>\n",
       "      <td>3</td>\n",
       "      <td>2020</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>23</td>\n",
       "      <td>58</td>\n",
       "      <td>42.29</td>\n",
       "      <td>Online Transaction</td>\n",
       "      <td>-7421093378627544099</td>\n",
       "      <td>ONLINE</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>5311</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24386900 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          user  card  year  month  day  hour  minute  amount  \\\n",
       "0          791     1  1991      1    2     7      10   68.00   \n",
       "1          791     1  1991      1    2     7      17  -68.00   \n",
       "2          791     1  1991      1    2     7      21  113.62   \n",
       "3          791     1  1991      1    2    17      30  114.73   \n",
       "4          791     1  1991      1    3     9       3  251.71   \n",
       "...        ...   ...   ...    ...  ...   ...     ...     ...   \n",
       "24386895  1659     2  2020      2   28    23      51    7.67   \n",
       "24386896   863     1  2020      2   28    23      53   49.06   \n",
       "24386897  1300     0  2020      2   28    23      56   51.29   \n",
       "24386898  1366     2  2020      2   28    23      56  132.73   \n",
       "24386899   446     3  2020      2   28    23      58   42.29   \n",
       "\n",
       "                    use chip        merchant name     merchant city  \\\n",
       "0          Swipe Transaction  2027553650310142703             Burke   \n",
       "1          Swipe Transaction  2027553650310142703             Burke   \n",
       "2          Swipe Transaction  2027553650310142703             Burke   \n",
       "3          Swipe Transaction -7269691894846892021             Burke   \n",
       "4          Swipe Transaction -3693650930986299431             Burke   \n",
       "...                      ...                  ...               ...   \n",
       "24386895    Chip Transaction  7231700044622779845             Cosby   \n",
       "24386896    Chip Transaction  7654254764356253071   North Brunswick   \n",
       "24386897  Online Transaction -6458444334611773637            ONLINE   \n",
       "24386898    Chip Transaction -7398558035733466800  Rockville Centre   \n",
       "24386899  Online Transaction -7421093378627544099            ONLINE   \n",
       "\n",
       "         merchant state    zip   mcc errors  is_fraud  \n",
       "0                    VA  22015  5541   <NA>         0  \n",
       "1                    VA  22015  5541   <NA>         0  \n",
       "2                    VA  22015  5541   <NA>         0  \n",
       "3                    VA  22015  5411   <NA>         0  \n",
       "4                    VA  22015  4814   <NA>         0  \n",
       "...                 ...    ...   ...    ...       ...  \n",
       "24386895             TN  37722  5300   <NA>         0  \n",
       "24386896             NJ  08902  5912   <NA>         0  \n",
       "24386897           <NA>   <NA>  4784   <NA>         0  \n",
       "24386898             NY  11570  5812   <NA>         0  \n",
       "24386899           <NA>   <NA>  5311   <NA>         0  \n",
       "\n",
       "[24386900 rows x 16 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "## Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete\n",
      "Elapsed (s):  236.001\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>card</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>amount</th>\n",
       "      <th>use chip</th>\n",
       "      <th>merchant name</th>\n",
       "      <th>merchant city</th>\n",
       "      <th>merchant state</th>\n",
       "      <th>zip</th>\n",
       "      <th>mcc</th>\n",
       "      <th>errors</th>\n",
       "      <th>is_fraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3505310</th>\n",
       "      <td>1761</td>\n",
       "      <td>1</td>\n",
       "      <td>2006</td>\n",
       "      <td>8</td>\n",
       "      <td>29</td>\n",
       "      <td>14</td>\n",
       "      <td>29</td>\n",
       "      <td>25.29</td>\n",
       "      <td>Online Transaction</td>\n",
       "      <td>31551052261259716</td>\n",
       "      <td>ONLINE</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>4784</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>716646</th>\n",
       "      <td>1520</td>\n",
       "      <td>4</td>\n",
       "      <td>2001</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>17</td>\n",
       "      <td>2.38</td>\n",
       "      <td>Swipe Transaction</td>\n",
       "      <td>-6571010470072147219</td>\n",
       "      <td>Norwalk</td>\n",
       "      <td>CT</td>\n",
       "      <td>06854</td>\n",
       "      <td>5499</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5558369</th>\n",
       "      <td>1590</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>7</td>\n",
       "      <td>23</td>\n",
       "      <td>6</td>\n",
       "      <td>49</td>\n",
       "      <td>27.82</td>\n",
       "      <td>Swipe Transaction</td>\n",
       "      <td>706166510299238548</td>\n",
       "      <td>Patterson</td>\n",
       "      <td>LA</td>\n",
       "      <td>70392</td>\n",
       "      <td>7538</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19519445</th>\n",
       "      <td>848</td>\n",
       "      <td>2</td>\n",
       "      <td>2017</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>42</td>\n",
       "      <td>679.31</td>\n",
       "      <td>Chip Transaction</td>\n",
       "      <td>-8436105799536235330</td>\n",
       "      <td>Yonkers</td>\n",
       "      <td>NY</td>\n",
       "      <td>10710</td>\n",
       "      <td>8111</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20362175</th>\n",
       "      <td>1897</td>\n",
       "      <td>0</td>\n",
       "      <td>2017</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>33</td>\n",
       "      <td>63.00</td>\n",
       "      <td>Chip Transaction</td>\n",
       "      <td>1799189980464955940</td>\n",
       "      <td>Boulder</td>\n",
       "      <td>CO</td>\n",
       "      <td>80301</td>\n",
       "      <td>5499</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          user  card  year  month  day  hour  minute  amount  \\\n",
       "3505310   1761     1  2006      8   29    14      29   25.29   \n",
       "716646    1520     4  2001     11    4    14      17    2.38   \n",
       "5558369   1590     2  2008      7   23     6      49   27.82   \n",
       "19519445   848     2  2017      5   16    12      42  679.31   \n",
       "20362175  1897     0  2017     11   10    14      33   63.00   \n",
       "\n",
       "                    use chip        merchant name merchant city  \\\n",
       "3505310   Online Transaction    31551052261259716        ONLINE   \n",
       "716646     Swipe Transaction -6571010470072147219       Norwalk   \n",
       "5558369    Swipe Transaction   706166510299238548     Patterson   \n",
       "19519445    Chip Transaction -8436105799536235330       Yonkers   \n",
       "20362175    Chip Transaction  1799189980464955940       Boulder   \n",
       "\n",
       "         merchant state    zip   mcc errors  is_fraud  \n",
       "3505310            None         4784   None         0  \n",
       "716646               CT  06854  5499   None         0  \n",
       "5558369              LA  70392  7538   None         0  \n",
       "19519445             NY  10710  8111   None         0  \n",
       "20362175             CO  80301  5499   None         0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# no need to run pandas, results already cached\n",
    "if False:\n",
    "    def preprocess_raw_data(fp: str, out_fp: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        preprocess the raw transaction data\n",
    "        fp(str): path to the raw transaction data.\n",
    "        \"\"\"\n",
    "        df = pd.read_parquet(fp)\n",
    "        df = df.rename(columns={'Errors?': 'errors',\n",
    "                                'Is Fraud?': 'is_fraud'\n",
    "                                })\n",
    "\n",
    "        # split time into hour and minute\n",
    "        df[['hour', 'minute']] = df.Time.str.split(':', expand=True)\n",
    "        df.hour = df.hour.astype(int)\n",
    "        df.minute = df.minute.astype(int)\n",
    "\n",
    "        # remove the 'Time' Column once it is parsed\n",
    "        del df['Time']\n",
    "        # rename all cols to lowercase\n",
    "        df.columns = [i.lower() for i in df.columns]\n",
    "        # add date col\n",
    "        df['date'] = pd.to_datetime(df[['year', 'month', 'day', 'hour', 'minute']])\n",
    "\n",
    "        # remove the dollar string and convert amount to float\n",
    "        df['amount'] = df['amount'].str.replace('$', '', regex=False).astype('float')\n",
    "\n",
    "        # sort rows by date and re-order the cols, exclude date column\n",
    "        df = df.sort_values('date')[['user', 'card', 'year', 'month', 'day', 'hour', 'minute', 'amount',\n",
    "                                     'use chip', 'merchant name', 'merchant city', 'merchant state', 'zip',\n",
    "                                     'mcc', 'errors', 'is_fraud']].reset_index(drop=True)\n",
    "\n",
    "        # fix the zip column\n",
    "        df['zip'] = df['zip'].apply(lambda x: '' if pd.isna(x) else \"{:05.0f}\".format(x))\n",
    "        # factorize is_fraud col into 1 and 0.\n",
    "        df['is_fraud'], fraud_key = df['is_fraud'].factorize()\n",
    "        df['use chip'] = df['use chip'].str.strip()\n",
    "        df['merchant city'] = df['merchant city'].str.strip()\n",
    "\n",
    "        df.to_parquet(out_fp)\n",
    "        print('Complete')\n",
    "        return df\n",
    "\n",
    "    # we'll save the \n",
    "    data_fp = './data/card_transaction.v1.parquet'\n",
    "    out_data_fp = './data/card_transactions_fixed.parquet'\n",
    "\n",
    "    start =time.time()\n",
    "    df = preprocess_raw_data(data_fp, out_data_fp)\n",
    "    print(f'Elapsed (s): {time.time() - start: 0.3f}')\n",
    "\n",
    "    df.sample(5, random_state=2718)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance times will vary based on your cpu architecture\n",
    "```\n",
    "Complete\n",
    "Elapsed (s):  236.001\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 6. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div><font size=\"4\">Tokenize the DataFrame columns</font></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "vocabulary_path = 'credit_card_coder.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user 0\n",
      "card 2000\n",
      "year 2009\n",
      "month 2039\n",
      "day 2051\n",
      "hour 2082\n",
      "minute 2106\n",
      "amount 2166\n",
      "use chip 2237\n",
      "merchant name 2240\n",
      "merchant city 102583\n",
      "merchant state 116012\n",
      "zip 116236\n",
      "mcc 143558\n",
      "errors 143667\n",
      "is_fraud 143691\n",
      "Each row uses 24 tokens\n"
     ]
    }
   ],
   "source": [
    "# the ordering of these columns is VERY IMPORTANT when we begin to pass context to our model during inference\n",
    "import cudf\n",
    "import pickle\n",
    "from coder.column_code import ColumnTokenizer, FloatTokenizer, CategoricalTokenizer\n",
    "\n",
    "if isinstance(df, cudf.DataFrame):\n",
    "    df = df.to_pandas()\n",
    "# filling missing rows with None\n",
    "df = df.fillna('None')\n",
    "\n",
    "column_codes_gpu = ColumnTokenizer()\n",
    "\n",
    "beg = 0\n",
    "cc = None\n",
    "\n",
    "#ACTION MAKE SURE THE COLUMNS ARE CORRECT HERE.\n",
    "FLOAT_COLS = ['amount']\n",
    "EXCLUDED_COLS = []\n",
    "columns = [col for col in df.columns if col not in EXCLUDED_COLS]\n",
    "\n",
    "\n",
    "for column in columns:\n",
    "    start_id = beg if cc is None else cc.end_id\n",
    "    print(column, start_id)\n",
    "    if column in FLOAT_COLS:\n",
    "        cc = FloatTokenizer(column, df[[column]], start_id, 'quantile')\n",
    "\n",
    "    else:\n",
    "        cc = CategoricalTokenizer(column, df[column], start_id)\n",
    "\n",
    "    column_codes_gpu.register(column, cc)\n",
    "\n",
    "# add 1 for newline char to separate each row\n",
    "print('Each row uses', sum(column_codes_gpu.sizes) + 1, 'tokens')\n",
    "\n",
    "# save the encoder and decoder\n",
    "with open(vocabulary_path, 'wb') as handle:\n",
    "    pickle.dump(column_codes_gpu, handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user:\t 1\n",
      "card:\t 1\n",
      "year:\t 1\n",
      "month:\t 1\n",
      "day:\t 1\n",
      "hour:\t 1\n",
      "minute:\t 1\n",
      "amount:\t 8\n",
      "use chip:\t 1\n",
      "merchant name:\t 1\n",
      "merchant city:\t 1\n",
      "merchant state:\t 1\n",
      "zip:\t 1\n",
      "mcc:\t 1\n",
      "errors:\t 1\n",
      "is_fraud:\t 1\n"
     ]
    }
   ],
   "source": [
    "for col, size in zip(column_codes_gpu.columns, column_codes_gpu.sizes):\n",
    "    print(f'{col}:\\t {size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from coder.column_code import ColumnTokenizer, FloatTokenizer, CategoricalTokenizer\n",
    "\n",
    "column_codes_gpu = ColumnTokenizer()\n",
    "with open(vocabulary_path, 'rb') as handle:\n",
    "    column_codes_gpu = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from coder.tabular_tokenizer import TabularTokenizer\n",
    "ENDOFTEXT = '<|endoftext|>'\n",
    "DELIMITER = '|'\n",
    "tokenizer = TabularTokenizer(vocabulary_path,\n",
    "                             special_tokens=['\\n', ENDOFTEXT],\n",
    "                             delimiter=DELIMITER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143693"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "<div><font size=\"4\">Let's try out our tokenizer on the `Amount` and `Merchant City` columns of the TabFormer Dataset.</font></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ids for 134.09 is: [2166, 2167, 2186, 2190, 2203, 2209, 2217, 2232]\n",
      "Recovered Amount for 134.09 is: 134.0897315256757\n",
      "\n",
      "Token ids for Monterey Park is: [103152]\n",
      "Recovered Merchant City for Monterey Park is: Monterey Park\n"
     ]
    }
   ],
   "source": [
    "float_str = '134.09'\n",
    "token_ids = column_codes_gpu.encode('amount', float_str)\n",
    "print('Token ids for {} is: {}'.format(float_str, token_ids))\n",
    "amt_str = column_codes_gpu.decode('amount', token_ids)\n",
    "print('Recovered Amount for {} is: {}\\n'.format(float_str, amt_str))\n",
    "\n",
    "city_str = 'Monterey Park'\n",
    "token_ids = column_codes_gpu.encode('merchant city', city_str)\n",
    "print('Token ids for {} is: {}'.format(city_str, token_ids))\n",
    "amt_str = column_codes_gpu.decode('merchant city', token_ids)\n",
    "print('Recovered Merchant City for {} is: {}'.format(city_str, amt_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We used a number of tokens for the float column, `Amount` and recovered the quantized amount with a small loss in accuracy.\n",
    "The `Merchant City` column only requires `1` token to encode `Monterey Park`.\n",
    "\n",
    "Now that we have the encoder and decoder implemented for tabular data, we can apply it to encode each of the columns in the dataset. Since the number of tokens per column is fixed, decoding the data is easy as the structure of the tabular data is maintained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "## 6.1 Modeling time series information with the `<|endoftext|>` token on the tokenized data\n",
    "\n",
    "To model the temporal information in the time series, we want to make sure the <|endoftext|> is added between the continuous sections. For example, in this credit card dataset, there are 2000 users. \n",
    "\n",
    "```python\n",
    "# There are 2000 users in this dataset.\n",
    "set(df.User.unique().to_numpy()) == set(range(2000))\n",
    "Out: True\n",
    "```\n",
    "\n",
    "Each user's transactions are one long time series sequence. We split the long time series into smaller overlapping pieces with some strike number. The <|endoftext|> is only added at the beginning and end of the long sequences but not in between. In this way, Megatron applies attention to learn the temporal correlation in the transactions for the user but not across users. \n",
    "\n",
    "We have provided the Python code to convert the CSV file into the loose json format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "## 6.2 Collate data by User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# converting time back to ints for sorting grouped rows by time so transactions are in order\n",
    "df[['year', 'month', 'day', 'hour', 'minute']] = df[['year', 'month', 'day', 'hour', 'minute']].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from typing import List\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "ENDOFTEXT = '<|endoftext|>'\n",
    "DELIMITER = '|'\n",
    "\n",
    "\n",
    "def get_docs_from_df(group) -> List:\n",
    "    \"\"\"\n",
    "    Make a document of size, WINDOW_SIZE rows from a user's transactions. The STRIDE moves the window by the provided number of rows.\n",
    "    \n",
    "    group: a DataFrame.groupby group\n",
    "    \"\"\"\n",
    "    WINDOW_SIZE = 500\n",
    "    STRIDE = 250\n",
    "    group = group.sort_values(['year', 'month', 'day', 'hour', 'minute'])\n",
    "    group = group.astype(str) # convert to string for concatenation in loop below\n",
    "\n",
    "    result = group[[\"user\"]].copy()\n",
    "    result.columns = [\"Out\"]\n",
    "    cols = group.columns[1:]\n",
    "    total = []\n",
    "    for col in cols:\n",
    "        # print(col)\n",
    "        result[\"Out\"] = result[\"Out\"].str.cat(group[col], sep=DELIMITER)\n",
    "    \n",
    "    for start_rows in range(0, max(len(result) - WINDOW_SIZE, len(result)), STRIDE):\n",
    "        if not start_rows:\n",
    "            total.append(json.dumps({'text': ENDOFTEXT + '\\n'.join(result['Out'].iloc[start_rows: start_rows + WINDOW_SIZE].to_json(orient='values')[2:-2].replace('\\\\', '').split('\",\"'))}) + '\\n')\n",
    "        else:\n",
    "            total.append(json.dumps({'text': '\\n'.join(result['Out'].iloc[start_rows: start_rows + WINDOW_SIZE].to_json(orient='values')[2:-2].replace('\\\\', '').split('\",\"'))}) + '\\n')\n",
    "\n",
    "    return total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Pool, cpu_count\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "# https://stackoverflow.com/questions/26187759/parallelize-apply-after-pandas-groupby?noredirect=1&lq=1\n",
    "def applyParallel(dfGrouped, func, convert_to_series=False):\n",
    "    with Pool(cpu_count()) as p:\n",
    "        ret_list = p.map(func, [group for name, group in dfGrouped])\n",
    "    if convert_to_series:\n",
    "        return pd.concat(ret_list)\n",
    "    return ret_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed (s):  82.131\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "# out = df.groupby('user').progress_apply(get_docs_from_df)\n",
    "out = applyParallel(df.groupby('user'), get_docs_from_df)\n",
    "print(f\"elapsed (s): {time.time() - start: 0.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In the code above, we `groupby user` and pass each group into `get_docs_from_df`. This function sorts the user's transactions by date, and then makes `\"documents\"` of length of 500 rows (`WINDOW_SIZE`) and with an temporal overlap of 250 rows (`STRIDE`). We add the special `<|endoftext|>` token to the beginning of the first document to mask the attention from another user's transactions. The variables `WINDOW_SIZE` and `STRIDE` are hyperparameters.\n",
    "\n",
    "Recall that Megatron expects a json lines format, with a document per line and the `text` field to contain the document (in this case, transactions) of interest.\n",
    "\n",
    "Thus we can take the DataFrame\n",
    "\n",
    "| user | card | amount | year | month | day  | hour | minute | use chip              | merchant name | merchant city | merchant state | zip   | mcc   | errors | is fraud |\n",
    "|------|------|------- |------|------ |------| -----| ------ | ----------------------| --------------| ------------- | -------------- | ----- | ---   | ------ | -------- |\n",
    "| 791  | 1    | 68.00  | 2018 |  1    |  2   |  9   |     10 |    Swipe Transaction  | 12345536      |  New York     | NY             | 10017 |  8005 |  \\<NA> | 0        |\n",
    "| 1572 | 0    | 572.42 | 2018 |  4    |  12  |  7   |     11 |    Chip Transaction   | 49908535      |  Princeton    | NJ             | 19406 |  5634 |  \\<NA> | 0        |\n",
    "| 2718 | 7    | 123.10 | 2019 |  1    |  4   |  10  |     14 |    Chip Transaction   | 43211536      |  Beverly Hills| CA             | 90210 |  4800 |  \\<NA> | 0        |\n",
    "| 21   | 2    | 42.04  | 2020 |  6    |  23  |  11  |     18 |    Swipe Transaction  | 65423006      |  Burke        | VA             | 22015 |  5604 |  \\<NA> | 0        |\n",
    "| 1001 | 1    | 5000.00| 2020 |  11   |  3   |  1   |     22 |    Online Transaction | 75434546      |  \\<NA>        | \\<NA>          | \\<NA> |  1234 |  \\<NA> | 1        |\n",
    "\n",
    "and transform it into documents:\n",
    "\n",
    "```json\n",
    "{'text': '<|endoftext|>0|0|2002|9|1|6|21|134.09|Swipe Transaction|3527213246127876953|La Verne|CA|91750|5300|None|No\\n0|0|2002|9|1|6|42|38.48|Swipe Transaction|-727612092139916043|Monterey Park|CA|91754|5411|None|No\\n0|0|2002|9|2|6|22|120.34|Swipe Transaction|-727612092139916043|Monterey Park|CA|91754|5411|None|No\\n0|0|2002|9|2|17|45|128.95|Swipe Transaction|3414527459579106770|Monterey Park|CA|91754|5651|None|No\\n\n",
    " ...},\n",
    "{'text': '0|0|2002|12|2|12|54|50.0|Swipe Transaction|-1288082279022882052|La Verne|CA|91750|5499|None|No\\n0|0|2002|12|2|19|8|71.03|Swipe Transaction|-4733023138943446282|Chicago|IL|60643|5812|None|No\\n0|0|2002|12|2|20|19|78.32|Swipe Transaction|-2744911404133435018|Chicago|IL|60645|5812|None|No\\n0|0|2002|12|2|23|22|127.0|Swipe Transaction|-6406662083475903219|Chicago|IL|60643|3390|None|No\\n0|0|2002|12|2|23|48|211.0|Swipe Transaction|-7807051024009846392|Peoria|IL|61604|3684|None|No\\n\n",
    " ...},\n",
    "...\n",
    "{'text': '<|endoftext|>1|2|2003|7|1|6|45|17.23|Swipe Transaction|-4791240532834014651|Little Neck|NY|11363|5921|None|No\\n1|2|2003|7|3|6|57|15.42|Swipe Transaction|1799189980464955940|Little Neck|NY|11363|5499|None|No\\n1|2|2003|7|4|5|22|22.23|Online Transaction|-521141999023077663|ONLINE|None|None|5815|Bad Expiration,|No\\n1|2|2003|7|4|5|29|10.28|Online Transaction|-521141999023077663|ONLINE|None|None|5815|None|No\\n\n",
    " ...},\n",
    "\n",
    "```\n",
    "Notice how the `<|endoftext|>` token separates user `0` transactions from user `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|endoftext|>0|0|2002|9|1|6|21|134.09|Swipe Transaction|3527213246127876953|La Verne|CA|91750|5300|None|0',\n",
       " '0|0|2002|9|1|6|42|38.48|Swipe Transaction|-727612092139916043|Monterey Park|CA|91754|5411|None|0',\n",
       " '0|0|2002|9|2|6|22|120.34|Swipe Transaction|-727612092139916043|Monterey Park|CA|91754|5411|None|0',\n",
       " '0|0|2002|9|2|17|45|128.95|Swipe Transaction|3414527459579106770|Monterey Park|CA|91754|5651|None|0',\n",
       " '0|0|2002|9|3|6|23|104.71|Swipe Transaction|5817218446178736267|La Verne|CA|91750|5912|None|0']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(out[0][0])['text'].split('\\n')[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 6.3 Save the document corpus to the `json lines` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:06<00:00, 293.92it/s]\n"
     ]
    }
   ],
   "source": [
    "jsonlines_path = 'credit_card_pd.jl'\n",
    "with open(jsonlines_path, 'w') as f:\n",
    "    for row in tqdm(out):\n",
    "        f.write(''.join(row))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Note we use '|' as delimiter because ',' is used as a character in the original file 'Errors' column. Having the loose JSON file ready, we can test the customized Tabular Tokenizer. You can use another delimiter as long as it is not present in the data, otherwise that would corrupt the output file when parsing the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from coder.tabular_tokenizer import TabularTokenizer\n",
    "\n",
    "\n",
    "tokenizer = TabularTokenizer(vocabulary_path,\n",
    "                             special_tokens=['\\n', ENDOFTEXT],\n",
    "                             delimiter=DELIMITER)\n",
    "\n",
    "with open(jsonlines_path, 'r') as f:\n",
    "    for line in f:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw input text\n",
      " <|endoftext|>0|0|2002|9|1|6|21|134.09|Swipe Transaction|3527213246127876953|La Verne|CA|91750|5300|None|0\n",
      "0|0|2002|9|1|6|42|38.48|Swipe Transaction|-727612092139916043|Monterey Park|CA|91754|5411|None|0\n",
      "0|0|2002|9|2|6|22|120.34|Swipe Transaction|-727612092139916043|Monterey Park|CA|91754|5411|None|0\n",
      "0|0|2002|9|2|17|45|128.95|Swipe Transaction|3414527459579106770|Monterey Park|CA|91754|5651|None|0\n",
      "0|0|2002|9|3|6|23|104.71|Swipe Transaction|5817218446178736267|La Verne|CA|91750|5912|None|0\n",
      "tokens\n",
      " ['<|endoftext|>', '0', '0', '2002', '9', '1', '6', '21', '134.09', 'Swipe Transaction', '3527213246127876953', 'La Verne', 'CA', '91750', '5300', 'None', '0', '\\n', '0', '0', '2002', '9', '1', '6', '42', '38.48', 'Swipe Transaction', '-727612092139916043', 'Monterey Park', 'CA', '91754', '5411', 'None', '0', '\\n', '0', '0', '2002', '9', '2', '6', '22', '120.34', 'Swipe Transaction', '-727612092139916043', 'Monterey Park', 'CA', '91754', '5411', 'None', '0', '\\n', '0', '0', '2002', '9', '2', '17', '45', '128.95', 'Swipe Transaction', '3414527459579106770', 'Monterey Park', 'CA', '91754', '5651', 'None', '0', '\\n', '0', '0', '2002', '9', '3', '6', '23', '104.71', 'Swipe Transaction', '5817218446178736267', 'La Verne']\n",
      "token ids\n",
      " [143694, 395, 2004, 2020, 2047, 2074, 2091, 2108, 2166, 2167, 2186, 2190, 2203, 2209, 2217, 2232, 2237, 4075, 106794, 116013, 123649, 143572, 143667, 143691, 143693, 395, 2004, 2020, 2047, 2074, 2091, 2157, 2166, 2167, 2182, 2194, 2199, 2211, 2219, 2236, 2237, 2330, 103152, 116013, 116978, 143559, 143667, 143691, 143693, 395, 2004, 2020, 2047, 2051, 2091, 2149, 2166, 2167, 2186, 2189, 2200, 2209, 2217, 2233, 2237, 2330, 103152, 116013, 116978, 143559, 143667, 143691, 143693, 395, 2004, 2020, 2047, 2051, 2083, 2154, 2166, 2167, 2186, 2190, 2198, 2213, 2226, 2230, 2237, 2440, 103152, 116013, 116978, 143584, 143667, 143691, 143693, 395, 2004, 2020, 2047, 2052, 2091, 2141, 2166, 2167, 2185, 2196, 2204, 2210, 2225, 2229, 2237, 2394, 106794, 116013, 123649, 143568, 143667, 143691]\n",
      "decoded tokens\n",
      " ['<|endoftext|>', '0', '0', '2002', '9', '1', '6', '21', '134.0897315256757', 'Swipe Transaction', '3527213246127876953', 'La Verne', 'CA', '91750', '5300', 'None', '0', '\\n', '0', '0', '2002', '9', '1', '6', '42', '38.479919939999995', 'Swipe Transaction', '-727612092139916043', 'Monterey Park', 'CA', '91754', '5411', 'None', '0', '\\n', '0', '0', '2002', '9', '2', '6', '22', '120.33942965783774', 'Swipe Transaction', '-727612092139916043', 'Monterey Park', 'CA', '91754', '5411', 'None', '0', '\\n', '0', '0', '2002', '9', '2', '17', '45', '128.9494810505405', 'Swipe Transaction', '3414527459579106770', 'Monterey Park', 'CA', '91754', '5651', 'None', '0', '\\n', '0', '0', '2002', '9', '3', '6', '23', '104.70963160864868', 'Swipe Transaction', '5817218446178736267', 'La Verne', 'CA', '91750', '5912', 'None', '0', '\\n', '0', '0', '2002', '9', '3', '13', '53', '86.18997584432427', 'Swipe Transaction', '-7146670748125200898', 'Monterey Park', 'CA', '91755', '5970', 'None', '0', '\\n', '0', '0', '2002', '9', '4', '5', '51', '93.83987955999999', 'Swipe Transaction', '-727612092139916043', 'Monterey Park', 'CA', '91754', '5411', 'None', '0', '\\n']\n",
      "decoded text\n",
      " <|endoftext|>0|0|2002|9|1|6|21|134.0897315256757|Swipe Transaction|3527213246127876953|La Verne|CA|91750|5300|None|0\n",
      "0|0|2002|9|1|6|42|38.479919939999995|Swipe Transaction|-727612092139916043|Monterey Park|CA|91754|5411|None|0\n",
      "0|0|2002|9|2|6|22|120.33942965783774|Swipe Transaction|-727612092139916043|Monterey Park|CA|91754|5411|None|0\n",
      "0|0|2002|9|2|17|45|128.9494810505405|Swipe Transaction|3414527459579106770|Monterey Park|CA|91754|5651|None|0\n",
      "0|0|2002|9|3|6|23|104.70963160864868|Swipe Transaction|5817218446178736267|La Verne|CA|91750|5912|None|0\n"
     ]
    }
   ],
   "source": [
    "text = json.loads(line)['text']\n",
    "r = tokenizer.tokenize_str(text)\n",
    "ids = tokenizer.convert_tokens_to_ids(r)\n",
    "tex = tokenizer.convert_ids_to_tokens(ids)\n",
    "decoded_text = tokenizer.decode(ids)\n",
    "show_num_lines = 5\n",
    "print('raw input text\\n', '\\n'.join(text.split('\\n')[:show_num_lines]))\n",
    "print('tokens\\n', r[:show_num_lines*len(tokenizer.code_column.columns)])\n",
    "print('token ids\\n', ids[:show_num_lines*(sum(tokenizer.code_column.sizes)+1)])\n",
    "print('decoded tokens\\n', tex[:show_num_lines*(sum(tokenizer.code_column.sizes)+1)])\n",
    "print('decoded text\\n', '\\n'.join(decoded_text.split('\\n')[:show_num_lines]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The TabularTokenizer understands the Table structure so it can convert back and forth between the tabular data text and the token ids."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "# 7 Model configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The <a href=\"./model_config.sh\">model_config.sh</a> file defines the model architecture and file paths\n",
    "\n",
    "```bash\n",
    "GPUS_PER_NODE=$(nvidia-smi -L | wc -l)\n",
    "# Change for multinode config\n",
    "MASTER_ADDR=localhost\n",
    "MASTER_PORT=6000\n",
    "NNODES=1\n",
    "NODE_RANK=0\n",
    "WORLD_SIZE=$(($GPUS_PER_NODE*$NNODES))\n",
    "\n",
    "DISTRIBUTED_ARGS=\"--nproc_per_node $GPUS_PER_NODE --nnodes $NNODES --node_rank $NODE_RANK --master_addr $MASTER_ADDR --master_port $MASTER_PORT\"\n",
    "\n",
    "\n",
    "OUTPUT_PATH=checkpoints\n",
    "PROJECT_NAME=creditcard\n",
    "INPUT_FILE=credit_card.jl\n",
    "DATA_PATH=${PROJECT_NAME}_text_document\n",
    "CHECKPOINT_PATH=${OUTPUT_PAHT}/gpt_${PROJECT_NAME}\n",
    "LOADPATH=${OUTPUT_PAHT}/gpt_${PROJECT_NAME}\n",
    "TB_PATH=${OUTPUT_PAHT}/checkpoints/tb\n",
    "\n",
    "TENSOR_MP_SIZE=1\n",
    "PIPELINE_MP_SIZE=1\n",
    "\n",
    "#model architecture\n",
    "NUM_LAYERS=48\n",
    "HIDDEN_SIZE=1024\n",
    "NUM_HEADS=16\n",
    "SEQ_LEN=2048\n",
    "MAX_POS_EMD=2048\n",
    "\n",
    "\n",
    "VOCAB_FILE=credit_card_coder.pickle\n",
    "TOKENIZER=TabularTokenizer\n",
    "PREPROCESS_WORKERS=$(nproc)\n",
    "```\n",
    "\n",
    "The model size can be adjusted by altering the parameters under `model architecture` section: \n",
    "```bash\n",
    "#model architecture\n",
    "NUM_LAYERS=48\n",
    "HIDDEN_SIZE=1024\n",
    "NUM_HEADS=16\n",
    "SEQ_LEN=2048\n",
    "MAX_POS_EMD=2048\n",
    "```\n",
    "\n",
    "The chart with different model size parameters is shown again below for your reference. Also recall, that the `TENSOR_MP_SIZE` and `PIPELINE_MP_SIZE` variables are passed to the `pretrain_step.sh` training script for convenience of using tensor and pipline parallelism. \n",
    "\n",
    "<center><img src=\"images/cases_april2021.png\" width=\"50%\" height=\"50%\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 7.1 Preprocess\n",
    "\n",
    "The loose json above is processed into a binary format for training. To convert the json into mmap (memory mapped), cached index file use `preprocess_data.py`. \n",
    "\n",
    "The `preprocess.sh` script to prepare data for GPT training:\n",
    "```bash\n",
    "#!/bin/bash\n",
    "source ./model_config.sh\n",
    "\n",
    "python tools/preprocess_data.py --input=$INPUT_FILE \\\n",
    "                                --output-prefix=$PROJECT_NAME \\\n",
    "                                --vocab=$VOCAB_FILE \\\n",
    "                                --dataset-impl=mmap \\\n",
    "                                        --tokenizer-type=$TOKENIZER \\\n",
    "                                --workers=$PREPROCESS_WORKERS\n",
    "\n",
    "```\n",
    "We defined the `PREPROCESS_WORKERS=nproc` parameter in the `model_config.sh`  workers to do the preprocess step in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "## 7.2 Run the preprocessing script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening credit_card_pd.jl\n",
      "> building TabularTokenizer tokenizer ...\n",
      " > padded vocab (size: 143695) with 49 dummy tokens (new size: 143744)\n",
      "> building TabularTokenizer tokenizer ...\n",
      "> building TabularTokenizer tokenizer ...\n",
      "> building TabularTokenizer tokenizer ...\n",
      "> building TabularTokenizer tokenizer ...\n",
      "> building TabularTokenizer tokenizer ...\n",
      "> building TabularTokenizer tokenizer ...\n",
      "> building TabularTokenizer tokenizer ...\n",
      "> building TabularTokenizer tokenizer ...\n",
      "> building TabularTokenizer tokenizer ...\n",
      "> building TabularTokenizer tokenizer ...\n",
      "> building TabularTokenizer tokenizer ...\n",
      "> building TabularTokenizer tokenizer ...\n",
      "> building TabularTokenizer tokenizer ...\n",
      "> building TabularTokenizer tokenizer ...\n",
      "> building TabularTokenizer tokenizer ...\n",
      "> building TabularTokenizer tokenizer ...\n",
      " > padded vocab (size: 143695) with 49 dummy tokens (new size: 143744)\n",
      " > padded vocab (size: 143695) with 49 dummy tokens (new size: 143744)\n",
      "> building TabularTokenizer tokenizer ...\n",
      " > padded vocab (size: 143695) with 49 dummy tokens (new size: 143744)\n",
      "> building TabularTokenizer tokenizer ...\n",
      " > padded vocab (size: 143695) with 49 dummy tokens (new size: 143744)\n",
      " > padded vocab (size: 143695) with 49 dummy tokens (new size: 143744)\n",
      " > padded vocab (size: 143695) with 49 dummy tokens (new size: 143744)\n",
      "> building TabularTokenizer tokenizer ...\n",
      " > padded vocab (size: 143695) with 49 dummy tokens (new size: 143744)\n",
      "> building TabularTokenizer tokenizer ...\n",
      " > padded vocab (size: 143695) with 49 dummy tokens (new size: 143744)\n",
      " > padded vocab (size: 143695) with 49 dummy tokens (new size: 143744)\n",
      "> building TabularTokenizer tokenizer ...\n",
      " > padded vocab (size: 143695) with 49 dummy tokens (new size: 143744)\n",
      " > padded vocab (size: 143695) with 49 dummy tokens (new size: 143744)\n",
      "> building TabularTokenizer tokenizer ...\n",
      " > padded vocab (size: 143695) with 49 dummy tokens (new size: 143744)\n",
      "> building TabularTokenizer tokenizer ...\n",
      "> building TabularTokenizer tokenizer ...\n",
      "Vocab size: 143695\n",
      "Output prefix: creditcard\n",
      " > padded vocab (size: 143695) with 49 dummy tokens (new size: 143744)\n",
      " > padded vocab (size: 143695) with 49 dummy tokens (new size: 143744)\n",
      " > padded vocab (size: 143695) with 49 dummy tokens (new size: 143744)\n",
      " > padded vocab (size: 143695) with 49 dummy tokens (new size: 143744)\n",
      " > padded vocab (size: 143695) with 49 dummy tokens (new size: 143744)\n",
      " > padded vocab (size: 143695) with 49 dummy tokens (new size: 143744)\n",
      " > padded vocab (size: 143695) with 49 dummy tokens (new size: 143744)\n",
      " > padded vocab (size: 143695) with 49 dummy tokens (new size: 143744)\n",
      " > padded vocab (size: 143695) with 49 dummy tokens (new size: 143744)\n",
      " > padded vocab (size: 143695) with 49 dummy tokens (new size: 143744)\n",
      " > padded vocab (size: 143695) with 49 dummy tokens (new size: 143744)\n",
      " > padded vocab (size: 143695) with 49 dummy tokens (new size: 143744)\n",
      "Time to startup: 1.2451462745666504\n",
      "Processed 98500 documents: : 98574it [06:39, 246.97it/s]\n"
     ]
    }
   ],
   "source": [
    "!./preprocess.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "After running the script, two new files are generated in the current directory: `creditcard_text_document.bin` and `creditcard_text_document.idx`. The `bin` file is the mmap binary file and the `idx` is the cached index file. They will be used for the following pre-training step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creditcard_text_document.bin  creditcard_text_document.idx\n"
     ]
    }
   ],
   "source": [
    "!ls creditcard_text_document.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 8. Please shut down the Kernel\n",
    "\n",
    "Ex. `Kernel -> Shut down kernel`, or in Jupyter Lab, navigating to the `Running Terminals and Kernels` Tab on the left sidebar, highlighting the mouse over this notebook's name in the `KERNELS` Section and selecting the `X` that appears."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# References\n",
    "\n",
    "<a id=\"1_1\">[1]</a> \n",
    "<a href=\"https://arxiv.org/pdf/1909.08053.pdf\">Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism</a>\n",
    "\n",
    "<a id=\"1_2\">[2]</a> \n",
    "<a href=\"https://arxiv.org/pdf/2104.04473.pdf\">Efficient Large-Scale Language Model Training on GPU Clusters\n",
    "Using Megatron-LM</a>\n",
    "\n",
    "<a id=\"1_3\">[3]</a> \n",
    "<a href=\"https://pytorch.org/docs/stable/distributed.html\">Distributed Training Overview</a>\n",
    "\n",
    "<a id=\"1_4\">[4]</a> \n",
    "<a href=\"https://pytorch.org/tutorials/intermediate/dist_tuto.html\">Distributed Training Tutorial</a>\n",
    "\n",
    "<a id=\"1_5\">[5]</a> \n",
    "<a href=\"https://github.com/pytorch/examples/tree/master/distributed/ddp\">Distributed Data Parallel Applications</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
